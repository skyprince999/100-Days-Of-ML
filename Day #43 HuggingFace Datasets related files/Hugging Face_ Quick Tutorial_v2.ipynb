{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.0.0-py3-none-any.whl (1.4 MB)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Using cached tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2020.11.13-cp37-cp37m-manylinux2014_x86_64.whl (719 kB)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: requests in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: six in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=d792b284197f498ea9a337a15e882318321327fa1fbff8254f0973f3243084a4\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, regex, sacremoses, transformers\n",
      "Successfully installed regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (0.9.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-1.1.3-py3-none-any.whl (153 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (4.42.1)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.18.1)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: pandas in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, dill, multiprocess, pyarrow, datasets\n",
      "Successfully installed datasets-1.1.3 dill-0.3.3 multiprocess-0.70.11.1 pyarrow-2.0.0 xxhash-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('glue', 'mrpc', split='train')\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-09548d7ff7317a7d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda example: example['label'] == dataset.features['label'].str2int('equivalent'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedSplit('train')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.microsoft.com/en-us/download/details.aspx?id=52398'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, builder_name='glue', config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943851, num_examples=3668, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105887, num_examples=408, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442418, num_examples=1725, dataset_name='glue')}, download_checksums={'https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': '971d7767d81b997fd9060ade0ec23c4fc31cbb226a55d1bd4a1bac474eb81dc7'}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': '60a9b09084528f0673eedee2b69cb941920f0b8cd0eeccefc464a98768457f89'}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': 'a04e271090879aaba6423d65b94950c089298587d9c084bf9cd7439bd785f784'}}, download_size=1494541, post_processing_size=None, dataset_size=1492156, size_in_bytes=2986697)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-ce6894453077b84f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idx': 1,\n",
       " 'label': 0,\n",
       " 'sentence1': \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       " 'sentence2': \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda example: example['label'] == dataset.features['label'].str2int('not_equivalent'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dataset in module datasets.arrow_dataset object:\n",
      "\n",
      "class Dataset(DatasetInfoMixin, datasets.search.IndexableMixin)\n",
      " |  Dataset(arrow_table: pyarrow.lib.Table, data_files: Union[List[dict], NoneType] = None, info: Union[datasets.info.DatasetInfo, NoneType] = None, split: Union[datasets.splits.NamedSplit, NoneType] = None, indices_table: Union[pyarrow.lib.Table, NoneType] = None, indices_data_files: Union[List[dict], NoneType] = None, fingerprint: Union[str, NoneType] = None, inplace_history: Union[List[dict], NoneType] = None)\n",
      " |  \n",
      " |  A Dataset backed by an Arrow table or Record Batch.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      DatasetInfoMixin\n",
      " |      datasets.search.IndexableMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __getitem__(self, key: Union[int, slice, str]) -> Union[Dict, List]\n",
      " |      Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, arrow_table: pyarrow.lib.Table, data_files: Union[List[dict], NoneType] = None, info: Union[datasets.info.DatasetInfo, NoneType] = None, split: Union[datasets.splits.NamedSplit, NoneType] = None, indices_table: Union[pyarrow.lib.Table, NoneType] = None, indices_data_files: Union[List[dict], NoneType] = None, fingerprint: Union[str, NoneType] = None, inplace_history: Union[List[dict], NoneType] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate through the examples.\n",
      " |      If a formatting is set with :func:`datasets.Dataset.set_format` rows will be returned with the\n",
      " |      selected format.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Number of rows in the dataset\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_elasticsearch_index(self, column: str, index_name: Union[str, NoneType] = None, host: Union[str, NoneType] = None, port: Union[int, NoneType] = None, es_client: Union[ForwardRef('elasticsearch.Elasticsearch'), NoneType] = None, es_index_name: Union[str, NoneType] = None, es_index_config: Union[dict, NoneType] = None)\n",
      " |      Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              The column of the documents to add to the index.\n",
      " |          index_name (Optional :obj:`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |              This is the index name that is used to call :func:`datasets.Dataset.get_nearest_examples` or :func:`datasets.Dataset.search`.\n",
      " |              By default it corresponds to :obj:`column`.\n",
      " |          host (Optional :obj:`str`, defaults to localhost):\n",
      " |              host of where ElasticSearch is running\n",
      " |          port (Optional :obj:`str`, defaults to 9200):\n",
      " |              port of where ElasticSearch is running\n",
      " |          es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n",
      " |              The elasticsearch client used to create the index if host and port are None.\n",
      " |          es_index_name (Optional :obj:`str`):\n",
      " |              The elasticsearch index name used to create the index.\n",
      " |          es_index_config (Optional :obj:`dict`):\n",
      " |              The configuration of the elasticsearch index.\n",
      " |              Default config is:\n",
      " |      \n",
      " |      Config::\n",
      " |      \n",
      " |          {\n",
      " |              \"settings\": {\n",
      " |                  \"number_of_shards\": 1,\n",
      " |                  \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
      " |              },\n",
      " |              \"mappings\": {\n",
      " |                  \"properties\": {\n",
      " |                      \"text\": {\n",
      " |                          \"type\": \"text\",\n",
      " |                          \"analyzer\": \"standard\",\n",
      " |                          \"similarity\": \"BM25\"\n",
      " |                      },\n",
      " |                  }\n",
      " |              },\n",
      " |          }\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          es_client = elasticsearch.Elasticsearch()\n",
      " |          ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |          ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name=\"my_es_index\")\n",
      " |          scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n",
      " |  \n",
      " |  add_faiss_index(self, column: str, index_name: Union[str, NoneType] = None, device: Union[int, NoneType] = None, string_factory: Union[str, NoneType] = None, metric_type: Union[int, NoneType] = None, custom_index: Union[ForwardRef('faiss.Index'), NoneType] = None, train_size: Union[int, NoneType] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
      " |      Add a dense index using Faiss for fast retrieval.\n",
      " |      By default the index is done over the vectors of the specified column.\n",
      " |      You can specify :obj:`device` if you want to run it on GPU (:obj:`device` must be the GPU index).\n",
      " |      You can find more information about Faiss here:\n",
      " |      \n",
      " |          - For `string factory <https://github.com/facebookresearch/faiss/wiki/The-index-factory>`__\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              The column of the vectors to add to the index.\n",
      " |          index_name (Optional :obj:`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |              This is the index_name that is used to call :func:`datasets.Dataset.get_nearest_examples` or :func:`datasets.Dataset.search`.\n",
      " |              By default it corresponds to `column`.\n",
      " |          device (Optional :obj:`int`):\n",
      " |              If not None, this is the index of the GPU to use.\n",
      " |              By default it uses the CPU.\n",
      " |          string_factory (Optional :obj:`str`):\n",
      " |              This is passed to the index factory of Faiss to create the index.\n",
      " |              Default index class is ``IndexFlat``.\n",
      " |          metric_type (Optional :obj:`int`):\n",
      " |              Type of metric. Ex: faiss.faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n",
      " |          custom_index (Optional :obj:`faiss.Index`):\n",
      " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
      " |          train_size (Optional :obj:`int`):\n",
      " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      " |          faiss_verbose (:obj:`bool`, defaults to False):\n",
      " |              Enable the verbosity of the Faiss index.\n",
      " |          dtype (data-type): The dtype of the numpy arrays that are indexed.\n",
      " |              Default is ``np.float32``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |          ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n",
      " |          ds_with_embeddings.add_faiss_index(column='embeddings')\n",
      " |          # query\n",
      " |          scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      " |          # save index\n",
      " |          ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
      " |      \n",
      " |          ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |          # load index\n",
      " |          ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
      " |          # query\n",
      " |          scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      " |  \n",
      " |  add_faiss_index_from_external_arrays(self, external_arrays: <built-in function array>, index_name: str, device: Union[int, NoneType] = None, string_factory: Union[str, NoneType] = None, metric_type: Union[int, NoneType] = None, custom_index: Union[ForwardRef('faiss.Index'), NoneType] = None, train_size: Union[int, NoneType] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
      " |      Add a dense index using Faiss for fast retrieval.\n",
      " |      The index is created using the vectors of `external_arrays`.\n",
      " |      You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n",
      " |      You can find more information about Faiss here:\n",
      " |      - For `string factory <https://github.com/facebookresearch/faiss/wiki/The-index-factory>`__\n",
      " |      \n",
      " |      Args:\n",
      " |          external_arrays (:obj:`np.array`):\n",
      " |              If you want to use arrays from outside the lib for the index, you can set :obj:`external_arrays`.\n",
      " |              It will use :obj:`external_arrays` to create the Faiss index instead of the arrays in the given :obj:`column`.\n",
      " |          index_name (:obj:`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |              This is the index_name that is used to call :func:`datasets.Dataset.get_nearest_examples` or :func:`datasets.Dataset.search`.\n",
      " |          device (Optional :obj:`int`):\n",
      " |              If not None, this is the index of the GPU to use.\n",
      " |              By default it uses the CPU.\n",
      " |          string_factory (Optional :obj:`str`):\n",
      " |              This is passed to the index factory of Faiss to create the index.\n",
      " |              Default index class is ``IndexFlat``.\n",
      " |          metric_type (Optional :obj:`int`):\n",
      " |              Type of metric. Ex: faiss.faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n",
      " |          custom_index (Optional :obj:`faiss.Index`):\n",
      " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
      " |          train_size (Optional :obj:`int`):\n",
      " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      " |          faiss_verbose (:obj:`bool`, defaults to False):\n",
      " |              Enable the verbosity of the Faiss index.\n",
      " |          dtype (:obj:`numpy.dtype`): The dtype of the numpy arrays that are indexed. Default is np.float32.\n",
      " |  \n",
      " |  cast_(self, features: datasets.features.Features)\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `feature` but :func:`cast_`\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          features (:class:`datasets.Features`): New features to cast the dataset to.\n",
      " |              The name of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n",
      " |  \n",
      " |  cleanup_cache_files(self)\n",
      " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n",
      " |      Be carefull when running this command that no other process is currently using other cache files.\n",
      " |      \n",
      " |      Return:\n",
      " |          Number of removed files\n",
      " |  \n",
      " |  dictionary_encode_column_(self, column: str)\n",
      " |      Dictionary encode a column.\n",
      " |      \n",
      " |          Dictionary encode can reduce the size of a column with many repetitions (e.g. string labels columns)\n",
      " |          by storing a dictionary of the strings. This only affect the internal storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |  \n",
      " |  export(self, filename: str, format: str = 'tfrecord')\n",
      " |      Writes the Arrow dataset to a TFRecord file.\n",
      " |      \n",
      " |      The dataset must already be in tensorflow format. The records will be written with\n",
      " |      keys from `dataset._format_columns`.\n",
      " |      \n",
      " |      Args:\n",
      " |          `filename` (`str`): The filename, including the .tfrecord extension, to write to.\n",
      " |          `format` (`Optional[str]`, default: `\"tfrecord\"`): The type of output file. Currently this is a no-op, as\n",
      " |              TFRecords are the only option. This enables a more flexible function signature\n",
      " |              later.\n",
      " |  \n",
      " |  filter(self, function: Union[Callable, NoneType] = None, with_indices=False, input_columns: Union[str, List[str], NoneType] = None, batch_size: Union[int, NoneType] = 1000, remove_columns: Union[List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, fn_kwargs: Union[dict, NoneType] = None, num_proc: Union[int, NoneType] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Union[str, NoneType] = None) -> 'Dataset'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - `function(example: Union[Dict, Any]) -> bool` if `with_indices=False`\n",
      " |              - `function(example: Union[Dict, Any], indices: int) -> bool` if `with_indices=True`\n",
      " |              If no function is provided, default to an always True function: lambda x: True\n",
      " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n",
      " |          num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |          suffix_template (`str`, defaults to \"_{rank:05d}_of_{num_proc:05d}\"): If cache_file_name is specified, then this suffix\n",
      " |              will be added at the end of the base name of each. For example, if cache_file_name is \"processed.arrow\", then for\n",
      " |              rank=1 and num_proc=4, the resulting file would be \"processed_00001_of_00004.arrow\" for the default suffix.\n",
      " |          new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  flatten_(self, max_depth=16)\n",
      " |      Flatten the Table.\n",
      " |      Each column with a struct type is flattened into one column per struct field.\n",
      " |      Other columns are left unchanged.\n",
      " |  \n",
      " |  flatten_indices(self, keep_in_memory: bool = False, cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, features: Union[datasets.features.Features, NoneType] = None, disable_nullable: bool = True, new_fingerprint: Union[str, NoneType] = None) -> 'Dataset'\n",
      " |      Create and cache a new Dataset by flattening the indices mapping.\n",
      " |      \n",
      " |      Args:\n",
      " |          keep_in_memory (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          features (`Optional[datasets.Features]`, default: `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, default: `True`): Allow null values in the table.\n",
      " |          new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  formatted_as(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      To be used in a `with` statement. Set __getitem__ return format (type and columns)\n",
      " |      \n",
      " |      Args:\n",
      " |          type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas']\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (Optional ``List[str]``): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  map(self, function: Union[Callable, NoneType] = None, with_indices: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Union[int, NoneType] = 1000, drop_last_batch: bool = False, remove_columns: Union[List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, features: Union[datasets.features.Features, NoneType] = None, disable_nullable: bool = False, fn_kwargs: Union[dict, NoneType] = None, num_proc: Union[int, NoneType] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Union[str, NoneType] = None) -> 'Dataset'\n",
      " |      Apply a function to all the elements in the table (individually or in batches)\n",
      " |      and update the table (if function does updated examples).\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - `function(example: Union[Dict, Any]) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n",
      " |              - `function(example: Union[Dict, Any], indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n",
      " |              - `function(batch: Union[Dict[List], List[Any]]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n",
      " |              - `function(batch: Union[Dict[List], List[Any]], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n",
      " |              If no function is provided, default to identity function: lambda x: x\n",
      " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      " |          batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          drop_last_batch (`bool`, default: `False`): Whether a last batch smaller than the batch_size should be\n",
      " |              dropped instead of being processed by the function.\n",
      " |          remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `True`): Disallow null values in the table.\n",
      " |          fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n",
      " |          num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |          suffix_template (`str`, defaults to \"_{rank:05d}_of_{num_proc:05d}\"): If cache_file_name is specified, then this suffix\n",
      " |              will be added at the end of the base name of each. For example, if cache_file_name is \"processed.arrow\", then for\n",
      " |              rank=1 and num_proc=4, the resulting file would be \"processed_00001_of_00004.arrow\" for the default suffix.\n",
      " |          new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  remove_columns_(self, column_names: Union[str, List[str]])\n",
      " |      Remove one or several column(s) in the dataset and\n",
      " |      the features associated to them.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n",
      " |  \n",
      " |  rename_column_(self, original_column_name: str, new_column_name: str)\n",
      " |      Rename a column in the dataset and move the features associated to the original column under the new column name.\n",
      " |      \n",
      " |      You can also rename a column using :func:`Dataset.map` with `remove_columns` but the present method:\n",
      " |          - takes care of moving the original features under the new column name.\n",
      " |          - doesn't copy the data to a new dataset and is thus much faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          original_column_name (:obj:`str`): Name of the column to rename.\n",
      " |          new_column_name (:obj:`str`): New name for the column.\n",
      " |  \n",
      " |  reset_format(self)\n",
      " |      Reset __getitem__ return format to python objects and all columns.\n",
      " |      \n",
      " |      Same as ``self.set_format()``\n",
      " |  \n",
      " |  save_to_disk(self, dataset_path: str)\n",
      " |      Save the dataset in a dataset directory\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_path (``str``): path of the dataset directory where the dataset will be saved to\n",
      " |  \n",
      " |  select(self, indices: collections.abc.Iterable, keep_in_memory: bool = False, indices_cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, new_fingerprint: Union[str, NoneType] = None) -> 'Dataset'\n",
      " |      Create a new dataset with rows selected following the list/array of indices.\n",
      " |      \n",
      " |      Args:\n",
      " |          `indices` (sequence, iterable, ndarray or Series): List or 1D-array of integer indices for indexing.\n",
      " |          `keep_in_memory` (`bool`, default: `False`): Keep the indices mapping in memory instead of writing it to a cache file.\n",
      " |          `indices_cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the\n",
      " |              indices mapping instead of the automatically generated cache file name.\n",
      " |          `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  set_format(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set __getitem__ return format (type and columns)\n",
      " |      \n",
      " |      Args:\n",
      " |          type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas']\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (Optional ``List[str]``): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  shard(self, num_shards: int, index: int, contiguous: bool = False, keep_in_memory: bool = False, indices_cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000) -> 'Dataset'\n",
      " |      Return the `index`-nth shard from dataset split into `num_shards` pieces.\n",
      " |      \n",
      " |      This shards deterministically. dset.shard(n, i) will contain all elements of dset whose\n",
      " |      index mod n = i.\n",
      " |      \n",
      " |      dset.shard(n, i, contiguous=True) will instead split dset into contiguous chunks,\n",
      " |      so it can be easily concatenated back together after processing. If n % i == l, then the\n",
      " |      first l shards will have length (n // i) + 1, and the remaining shards will have length (n // i).\n",
      " |      `datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` will return\n",
      " |      a dataset with the same order as the original.\n",
      " |      \n",
      " |      Be sure to shard before using any randomizing operator (such as shuffle).\n",
      " |      It is best if the shard operator is used early in the dataset pipeline.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          num_shards (`int`): How many shards to split the dataset into.\n",
      " |          index (`int`): Which shard to select and return.\n",
      " |          contiguous: (`bool`, defaults to `False`): Whether to select contiguous blocks of indices for shards.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              indices of each shard instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |  \n",
      " |  shuffle(self, seed: Union[int, NoneType] = None, generator: Union[numpy.random._generator.Generator, NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, new_fingerprint: Union[str, NoneType] = None) -> 'Dataset'\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |      \n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |      \n",
      " |      Args:\n",
      " |          seed (Optional `int`): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |          generator (Optional `np.random.Generator`): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the shuffled indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the shuffled indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              shuffled indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  sort(self, column: str, reverse: bool = False, kind: str = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, new_fingerprint: Union[str, NoneType] = None) -> 'Dataset'\n",
      " |      Create a new dataset sorted according to a column.\n",
      " |      \n",
      " |      Currently sorting according to a column name uses numpy sorting algorithm under the hood.\n",
      " |      The column should thus be a numpy compatible type (in particular not a nested type).\n",
      " |      This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`): column name to sort by.\n",
      " |          reverse: (`bool`, defaults to `False`): If True, sort by descending order rather then ascending.\n",
      " |          kind (Optional `str`): Numpy algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n",
      " |              The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n",
      " |              the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the sorted indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the sorted indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              sorted indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      " |          new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  train_test_split(self, test_size: Union[float, int, NoneType] = None, train_size: Union[float, int, NoneType] = None, shuffle: bool = True, seed: Union[int, NoneType] = None, generator: Union[numpy.random._generator.Generator, NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, train_indices_cache_file_name: Union[str, NoneType] = None, test_indices_cache_file_name: Union[str, NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, train_new_fingerprint: Union[str, NoneType] = None, test_new_fingerprint: Union[str, NoneType] = None) -> 'DatasetDict'\n",
      " |      Return a dictionary (:obj:`datasets.DatsetDict`) with two random train and test subsets (`train` and `test` ``Dataset`` splits).\n",
      " |      Splits are created from the dataset according to `test_size`, `train_size` and `shuffle`.\n",
      " |      \n",
      " |      This method is similar to scikit-learn `train_test_split` with the omission of the stratified options.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_size (Optional `np.random.Generator`): Size of the test split\n",
      " |              If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\n",
      " |              If int, represents the absolute number of test samples.\n",
      " |              If None, the value is set to the complement of the train size.\n",
      " |              If train_size is also None, it will be set to 0.25.\n",
      " |          train_size (Optional `np.random.Generator`): Size of the train split\n",
      " |              If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |              If int, represents the absolute number of train samples.\n",
      " |              If None, the value is automatically set to the complement of the test size.\n",
      " |          shuffle (Optional `bool`, defaults to `True`): Whether or not to shuffle the data before splitting.\n",
      " |          seed (Optional `int`): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |          generator (Optional `np.random.Generator`): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the splits indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the splits indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          train_cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              train split indices instead of the automatically generated cache file name.\n",
      " |          test_cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              test split indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          train_new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the train set after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |          test_new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the test set after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  unique(self, column: str) -> List[Any]\n",
      " |      Return a list of the unique elements in a column.\n",
      " |      \n",
      " |      This is implemented in the low-level backend and as such, very fast.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              column name (list all the column names with :func:`datasets.Dataset.column_names`)\n",
      " |      \n",
      " |      Returns: :obj:`list` of unique elements in the given column.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_buffer(buffer: pyarrow.lib.Buffer, info: Union[datasets.info.DatasetInfo, NoneType] = None, split: Union[datasets.splits.NamedSplit, NoneType] = None, indices_buffer: Union[pyarrow.lib.Buffer, NoneType] = None) -> 'Dataset' from builtins.type\n",
      " |      Instantiate a Dataset backed by an Arrow buffer\n",
      " |  \n",
      " |  from_dict(mapping: dict, features: Union[datasets.features.Features, NoneType] = None, info: Union[Any, NoneType] = None, split: Union[Any, NoneType] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert :obj:``dict`` to a \"obj\"``pyarrow.Table`` to create a :obj:``datasets.Dataset``.\n",
      " |      \n",
      " |      Args:\n",
      " |          mapping (:obj:``mapping``): A mapping of strings to Arrays or Python lists.\n",
      " |          features (:obj:``datasets.Features``, `optional`, defaults to :obj:``None``): If specified, the features types of the dataset\n",
      " |          info (:obj:``datasets.DatasetInfo``, `optional`, defaults to :obj:``None``): If specified, the dataset info containing info like\n",
      " |              description, citation, etc.\n",
      " |          split (:obj:``datasets.NamedSplit``, `optional`, defaults to :obj:``None``): If specified, the name of the dataset split.\n",
      " |  \n",
      " |  from_file(filename: str, info: Union[datasets.info.DatasetInfo, NoneType] = None, split: Union[datasets.splits.NamedSplit, NoneType] = None, indices_filename: Union[str, NoneType] = None) -> 'Dataset' from builtins.type\n",
      " |      Instantiate a Dataset backed by an Arrow table at filename\n",
      " |  \n",
      " |  from_pandas(df: pandas.core.frame.DataFrame, features: Union[datasets.features.Features, NoneType] = None, info: Union[datasets.info.DatasetInfo, NoneType] = None, split: Union[datasets.splits.NamedSplit, NoneType] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert :obj:``pandas.DataFrame`` to a \"obj\"``pyarrow.Table`` to create a :obj:``datasets.Dataset``.\n",
      " |      \n",
      " |      The column types in the resulting Arrow Table are inferred from the dtypes of the pandas.Series in the DataFrame. In the case of non-object\n",
      " |      Series, the NumPy dtype is translated to its Arrow equivalent. In the case of `object`, we need to guess the datatype by looking at the\n",
      " |      Python objects in this Series.\n",
      " |      \n",
      " |      Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow type. In the case that\n",
      " |      we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to\n",
      " |      null. This behavior can be avoided by constructing explicit features and passing it to this function.\n",
      " |      \n",
      " |      Args:\n",
      " |          df (:obj:``pandas.DataFrame``): the dataframe that contains the dataset.\n",
      " |          features (:obj:``datasets.Features``, `optional`, defaults to :obj:``None``): If specified, the features types of the dataset\n",
      " |          info (:obj:``datasets.DatasetInfo``, `optional`, defaults to :obj:``None``): If specified, the dataset info containing info like\n",
      " |              description, citation, etc.\n",
      " |          split (:obj:``datasets.NamedSplit``, `optional`, defaults to :obj:``None``): If specified, the name of the dataset split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  load_from_disk(dataset_path: str) -> 'Dataset'\n",
      " |      Load the dataset from a dataset directory\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_path (``str``): path of the dataset directory where the dataset will be loaded from\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cache_files\n",
      " |      The cache file containing the Apache Arrow table backing the dataset.\n",
      " |  \n",
      " |  column_names\n",
      " |      Names of the columns in the dataset.\n",
      " |  \n",
      " |  data\n",
      " |      The Apache Arrow table backing the dataset.\n",
      " |  \n",
      " |  format\n",
      " |  \n",
      " |  num_columns\n",
      " |      Number of columns in the dataset.\n",
      " |  \n",
      " |  num_rows\n",
      " |      Number of rows in the dataset (same as :func:`datasets.Dataset.__len__`).\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of the dataset (number of columns, number of rows).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetInfoMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  builder_name\n",
      " |  \n",
      " |  citation\n",
      " |  \n",
      " |  config_name\n",
      " |  \n",
      " |  dataset_size\n",
      " |  \n",
      " |  description\n",
      " |  \n",
      " |  download_checksums\n",
      " |  \n",
      " |  download_size\n",
      " |  \n",
      " |  features\n",
      " |  \n",
      " |  homepage\n",
      " |  \n",
      " |  info\n",
      " |      :class:`datasets.DatasetInfo` object containing all the metadata in the dataset.\n",
      " |  \n",
      " |  license\n",
      " |  \n",
      " |  size_in_bytes\n",
      " |  \n",
      " |  split\n",
      " |      :class:`datasets.DatasetInfo` object containing all the metadata in the dataset.\n",
      " |  \n",
      " |  supervised_keys\n",
      " |  \n",
      " |  version\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from datasets.search.IndexableMixin:\n",
      " |  \n",
      " |  drop_index(self, index_name: str)\n",
      " |      Drop the index with the specified column.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |  \n",
      " |  get_index(self, index_name: str) -> datasets.search.BaseIndex\n",
      " |      List the index_name/identifiers of all the attached indexes.\n",
      " |  \n",
      " |  get_nearest_examples(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10) -> datasets.search.NearestExamplesResults\n",
      " |      Find the nearest examples in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |          query (:obj:`Union[str, np.ndarray]`): The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve.\n",
      " |      \n",
      " |      Ouput:\n",
      " |          scores (:obj:`List[float]`): The retrieval scores of the retrieved examples.\n",
      " |          examples (:obj:`dict`): The retrieved examples.\n",
      " |  \n",
      " |  get_nearest_examples_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10) -> datasets.search.BatchedNearestExamplesResults\n",
      " |      Find the nearest examples in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |          queries (:obj:`Union[List[str], np.ndarray]`): The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve per query.\n",
      " |      \n",
      " |      Ouput:\n",
      " |          total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n",
      " |          total_examples (`List[dict]`): The retrieved examples per query.\n",
      " |  \n",
      " |  is_index_initialized(self, index_name: str) -> bool\n",
      " |  \n",
      " |  list_indexes(self) -> List[str]\n",
      " |      List the colindex_nameumns/identifiers of all the attached indexes.\n",
      " |  \n",
      " |  load_elasticsearch_index(self, index_name: str, es_index_name: str, host: Union[str, NoneType] = None, port: Union[int, NoneType] = None, es_client: Union[ForwardRef('Elasticsearch'), NoneType] = None, es_index_config: Union[dict, NoneType] = None)\n",
      " |      Load an existing text index using ElasticSearch for fast retrieval.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index. This is the index name that is used to call `.get_nearest` or `.search`.\n",
      " |          es_index_name (`:obj:str`): The name of elasticsearch index to load.\n",
      " |          host (Optional :obj:`str`, defaults to localhost):\n",
      " |              host of where ElasticSearch is running\n",
      " |          port (Optional `:obj:str`, defaults to 9200):\n",
      " |              port of where ElasticSearch is running\n",
      " |          es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n",
      " |              The elasticsearch client used to create the index if host and port are None.\n",
      " |          es_index_config (Optional :obj:`dict`):\n",
      " |              The configuration of the elasticsearch index.\n",
      " |              Default config is:\n",
      " |      \n",
      " |      Config::\n",
      " |      \n",
      " |          {\n",
      " |              \"settings\": {\n",
      " |                  \"number_of_shards\": 1,\n",
      " |                  \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
      " |              },\n",
      " |              \"mappings\": {\n",
      " |                  \"properties\": {\n",
      " |                      \"text\": {\n",
      " |                          \"type\": \"text\",\n",
      " |                          \"analyzer\": \"standard\",\n",
      " |                          \"similarity\": \"BM25\"\n",
      " |                      },\n",
      " |                  }\n",
      " |              },\n",
      " |          }\n",
      " |  \n",
      " |  load_faiss_index(self, index_name: str, file: str, device: Union[int, NoneType] = None)\n",
      " |      Load a FaissIndex from disk.\n",
      " |      If you want to do additional configurations, you can have access to the faiss index object by doing `.get_index(index_name).faiss_index` to make it fit your needs\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n",
      " |          file (:obj:`str`): The path to the serialized faiss index on disk.\n",
      " |          device (Optional :obj:`int`): If not None, this is the index of the GPU to use. By default it uses the CPU.\n",
      " |  \n",
      " |  save_faiss_index(self, index_name: str, file: str)\n",
      " |      Save a FaissIndex on disk\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n",
      " |          file (:obj:`str`): The path to the serialized faiss index on disk.\n",
      " |  \n",
      " |  search(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10) -> datasets.search.SearchResults\n",
      " |      Find the nearest examples indices in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The name/identifier of the index.\n",
      " |          query (:obj:`Union[str, np.ndarray]`): The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve.\n",
      " |      \n",
      " |      Ouput:\n",
      " |          scores (:obj:`List[List[float]`): The retrieval scores of the retrieved examples.\n",
      " |          indices (:obj:`List[List[int]]`): The indices of the retrieved examples.\n",
      " |  \n",
      " |  search_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10) -> datasets.search.BatchedSearchResults\n",
      " |      Find the nearest examples indices in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |          queries (:obj:`Union[List[str], np.ndarray]`): The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (`:obj:int`): The number of examples to retrieve per query.\n",
      " |      \n",
      " |      Ouput:\n",
      " |          total_scores (:obj:`List[List[float]`): The retrieval scores of the retrieved examples per query.\n",
      " |          total_indices (:obj:`List[List[int]]`): The indices of the retrieved examples per query.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of bytes allocated on the drive is 2986697\n",
      "For comparison, here is the number of bytes allocated in memory: 0\n"
     ]
    }
   ],
   "source": [
    "# To see how large our dataset is\n",
    "\n",
    "from datasets import total_allocated_bytes\n",
    "\n",
    "print(\"The number of bytes allocated on the drive is\", dataset.size_in_bytes)\n",
    "print(\"For comparison, here is the number of bytes allocated in memory:\", total_allocated_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8159b7510f344e2289f4635f5f833ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6224144739fe4973a56d2f6cad886626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e4eb4638c84d4b8368afb8d4a3d5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e928de56b404feb9c5b15f7b3d0a1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 11336, 6732, 3384, 1106, 1140, 1112, 1178, 107, 1103, 7737, 107, 117, 7277, 2180, 5303, 4806, 1117, 1711, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(dataset[0]['sentence1'], dataset[0]['sentence2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Amrozi accused his brother, whom he called \" the witness \", of deliberately distorting his evidence. [SEP] Referring to him as only \" the witness \", Amrozi accused his brother of deliberately distorting his evidence. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(dataset[0]['sentence1'], dataset[0]['sentence2'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71b557550a943dd8a6e6edc7484161a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 0, 'input_ids': [101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 11336, 6732, 3384, 1106, 1140, 1112, 1178, 107, 1103, 7737, 107, 117, 7277, 2180, 5303, 4806, 1117, 1711, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d1b82ac13c4a68ba76af68511a0ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'input_ids': tensor([[  101,  7277,  2180,  ...,     0,     0,     0],\n",
       "         [  101, 10684,  2599,  ...,     0,     0,     0],\n",
       "         [  101,  1220,  1125,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1109,  2026,  ...,     0,     0,     0],\n",
       "         [  101, 22263,  1107,  ...,     0,     0,     0],\n",
       "         [  101,   142,  1813,  ...,     0,     0,     0]]),\n",
       " 'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "         1, 1, 0, 0, 1, 1, 1, 0]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/115 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.train().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-33953f6ff53a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
