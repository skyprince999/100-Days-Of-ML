{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./hugging/lib/python3.7/site-packages\")\n",
    "#sys.path.append(\"./datasets/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset ted_talks_iwslt/szl_rup (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/ubuntu/.cache/huggingface/datasets/ted_talks_iwslt/szl_rup/1.1.0/800eb7ea36ca4583056351b415c0bf0d459abae856c4052968a6e133d753ce55...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Dataset ted_talks_iwslt downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/ted_talks_iwslt/szl_rup/1.1.0/800eb7ea36ca4583056351b415c0bf0d459abae856c4052968a6e133d753ce55. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('datasets/datasets/ted_talks_iwslt',  \"szl_rup\",data_dir= \"wit/XML_releases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('translation',\n",
       "              [{'rup': 'Cu ajutorul câtorva scene surprinzatoare, Derec Sivers explică modul în care mișcarile iau naștere cu adevarat. ( Sugestie: este nevoie de doi. )',\n",
       "                'szl': 'Derek Sivers gŏdo, jako sie rŏbi firer a jigo czelŏtka (trza dwōch).'},\n",
       "               {'rup': 'Derek Sivers: Cum să începi o acțiune',\n",
       "                'szl': 'Jako pŏra ludzi zbiyro sie we czelŏtka, keby cosik napŏchać'}])])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ted_multi_translate (/home/ubuntu/.cache/huggingface/datasets/ted_multi_translate/plain_text/1.0.0/7a971d7e0a0ff960e3670bdfd8cdcfc1f5a54d8794fac77e87b150e80b5c7794)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('reviews/datasets/datasets/ted_multi', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'talk_name': 'jonas_gahr_store_in_defense_of_dialogue',\n",
       " 'translations': {'language': ['ar',\n",
       "   'bg',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fr',\n",
       "   'fr-ca',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'ko',\n",
       "   'nb',\n",
       "   'nl',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'pt-br',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sq',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh-cn',\n",
       "   'zh-tw'],\n",
       "  'translation': ['من ضمن جميع المثبطات المقلقة التي نعاني منها اليوم نفكر في المقام الاول في الامور المالية والاقتصادية واكثر ما يهمني بشكل اكثر هو عجز الحوار السياسي — قدرتنا على فهم الصراعات الحديثة على ماهي عليه , بالذهاب الى اصلها الفعلي وعلى فهم اللاعبين الرئيسيين وعلى التعامل معهم',\n",
       "   'Наред с всички обезпокоителни дефицити , с които се сблъскваме днес - ние основно мислим за финансовите и икономическите - този , който ме безпокои най-вече е липсата на политически диалог - нашата способност да подходим към съвременните конфликти както те присъстват , да стигнем до източника на това , от което те произтичат и да разберем ключовите участници и да се разберем с тях .',\n",
       "   'Unter den schwierigen Problemen , mit denen wir heutzutage ringen – wir denken hier in erster Linie an finanzielle und ökonomische Probleme – dasjenige , das mich am meisten beunruhigt , ist der Mangel an politischem Dialog : Unsere Fähigkeit , mit modernen Konflikten umzugehen , zur ihrer eigentlichen Quelle zu gehen , die Hauptakteure zu verstehen , und mit ihnen umzugehen .',\n",
       "   'Ανάμεσα σε όλα τα ανησυχητικά ελλείμματα που αντιμετωπίζουμε σήμερα - σκεφτόμαστε το πολιτικό και οικονομικό έλλειμμα αρχικά - αυτό που με ανησυχεί περισσότερο είναι το έλλειμμα του πολιτικού διαλόγου , η ικανότητά μας να αντιμετωπίσουμε σύγχρονες συγκρούσεις όπως είναι , να πάμε στην πηγή της αιτίας τους και να καταλάβουμε τους παίκτες-κλειδιά και να τους αντιμετωπίσουμε .',\n",
       "   'Amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they &apos;re all about and to understand the key players and to deal with them .',\n",
       "   'De todos los déficits preocupantes a los que nos enfrentamos hoy , pensamos principalmente en el financiero y en el económico . Pero el que más me preocupa es el déficit del diálogo político , nuestra capacidad para tratar conflictos modernos tal y como son , ir a la raíz de por qué existen y entender a los actores principales y tratar con ellos .',\n",
       "   'Gaur egun aurrean ditugun gabezia kezkagarri guztietatik , nagusiki finantza eta ekonomian pentsatzen dugu , ni gehien kezkatzen nauena elkarrizketa politiko falta da ordea . Gatazka modernoak direna bezala jorratzeko gaitasuna , gatazkaren errora iristea eta aktore nagusiak ulertzea eta haiekin tratatzea .',\n",
       "   'از میان همه کاستی های نگران کننده ای که امروزه با آنها درگیریم — بیشتر اوقات ذهنمان معطوف جنبه های مالی و اقتصادی است — آنچه مرا بیش از همه نگران می کند کمبود گفتگوی سیاسی است — یعنی توانایی ما در پرداختن به درگیری های مدرن آنگونه که هستند ، در ریشه یابی آنها و شناخت و درک بازیگران کلیدی و پرداختن به آنها .',\n",
       "   'Parmi tous les déficits inquiétants contre lesquels nous luttons aujourd&apos; hui — surtout en matière économique et financière — celui qui me préoccupe le plus est le déficit du dialogue politique — notre capacité à résoudre les conflits modernes tels qu&apos; ils sont , d&apos; aller à la source de ce qu&apos; ils sont et d&apos; identifier les principaux acteurs et de traiter avec eux .',\n",
       "   'Parmi tous les déficits que nous accusons aujourd &apos; hui , et même si l &apos; on pense qu &apos; ils sont surtout d &apos; ordre financier et économique , celui qui me préoccupe le plus , c &apos; est le déficit du dialogue politique ; C &apos; est-à-dire notre capacité à gérer les conflits modernes tels qu &apos; ils sont , de pouvoir comprendre leur source même et les acteurs clés qui les composent , pour ensuite s &apos; en occuper .',\n",
       "   'מתוך מכלול הגרעונות המציקים שעמם אנחנו מתמודדים היום אנחנו חושבים בראש וראשונה על הגרעון הפיננסי והכלכלי — הגרעון שמדאיג אותי יותר מכל הוא המחסור או ההעדר בדיאלוג פוליטי — שמשפיע על היכולת שלנו להתמודד עם סכסוכים מודרניים כמו שהם , לרדת לשורש העניין , להבין מי הם השחקנים העיקריים ולהתמודד איתם .',\n",
       "   'Među svim zabrinjavajućim problemima s kojima se danas susrećemo , tu prvenstveno mislimo na ekonomske i financijske , ali oni koji me najviše zabrinjavaju nedostatci su političkog dijaloga , naše sposobnosti da se nosimo s modernim sukobima kao takvima , da doprijemo do njihove biti i da shvatimo ključne igrače te da se s njima obračunamo .',\n",
       "   'Az összes aggasztó hiány közül , amikkel ma küszködünk — — elsősorban pénzügyi és a gazdasági hiányra gondolunk — engem leginkább a politikai párbeszéd hiánya aggaszt — azé a képességünké , hogy a modern konfliktusokat , annak vegyük , amik , hogy visszamenjünk az eredetükig , amiről valójában szólnak , és hogy megértsük a kulcsszereplőket és kezeljük őket .',\n",
       "   'Tra tutti i problematici disavanzi contro i quali combattiamo oggi — principalmente pensiamo a quelli finanziari o economici — quello che mi preoccupa maggiormente è il deficit del dialogo politico — la nostra capacità di indirizzarci ai conflitti moderni nel modo in cui sono , per andare alla fonte di ciò che riguardano , per capire chi sono i giocatori chiave e per trattare con loro .',\n",
       "   '我々が今日直面している 様々な機能不全のなかで — 財政や経済が最初に思いつきますが — 私が一番 憂慮しているのは 政治的対話の欠乏です 我々が 近年の紛争において 状況を把握し その根本原因を探り 中心人物を理解し 彼らと交渉をする能力です',\n",
       "   '오늘날 우리가 겪고 있는 문제를 야기하는 모든 결핍들 중에서 우리는 주로 재정과 경제를 생각합니다만 , 제가 가장 우려하는 것들은 정치적 대화의 결핍입니다 . 우리가 현대의 갈등들을 그 자체로서 논하고 , 그 갈등들이 진정 무엇에 관한것이었는지 그 근원을 알아보고 , 그 핵심 인물들을 이해하고 , 그리고 그들을 상대하는 능력말입니다 .',\n",
       "   'Blant alle de urovekkende manglene vi strever med i dag — vi tenker primært på finansielle og økonomiske — de som bekymrer meg mest er mangelen på politisk dialog — vår evne til å adressere moderne konflikter slik de er , å finne kilden for det de egentlig handler om og å forstå nøkkelspillerne og å forholde seg til dem .',\n",
       "   'Van al onze verontrustende tekorten tegenwoordig — als eerste denk je aan financiële en economische — is het meest belangrijke voor mij het tekort aan politieke dialoog : ons onvermogen met moderne conflicten om te gaan , naar de kern te gaan en de sleutelfiguren te begrijpen , en ze aan te pakken .',\n",
       "   'Wśród problemów z jakimi zmagamy się dziś , mamy na myśli przede wszystkim te finansowe i ekonomiczne . Problemem , który mnie interesuje najbardziej jest deficyt politycznego dialogu , brak umiejętności odniesienia się do współczesnych konfliktów , dotarcia do ich źródła i zrozumienia kim są ich kluczowi gracze i jak z nimi postępować .',\n",
       "   'Entre todas as grandes privações com que nos debatemos hoje — pensamos em financeiras e económicas primeiro — aquela que mais me preocupa é a falta de diálogo político — a nossa capacidade de abordar conflitos modernos como eles são , de ir à raiz do que eles são e perceber os agentes-chave e lidar com eles .',\n",
       "   'Entre todos os déficits preocupantes com que lidamos hoje — nós pensamos principalmente nos financeiros ou econômicos — o que me preocupa mais é a falta de diálogo político — nossa habilidade de lidar com os conflitos modernos como eles são , ir à fonte do que eles tratam , entender os atores chave e lidar com eles',\n",
       "   'Printre toate deficitele îngrijorătoare cu care ne luptăm azi — ne gândim la cele financiare şi economice în primul rând — cel care mă preocupă cel mai mult este deficitul dialogului politic — abilitatea noastră de a aborda conflictele moderne aşa cum se prezintă , de a ajunge la sursa care le provoacă să înţelegem jucătorii cheie şi să le facem faţă .',\n",
       "   'Среди всех дефицитов , беспокоящих нас сегодня — прежде всего финансового и экономического — меня больше всего волнует дефицит политического диалога : нашей способности реагировать на современные конфликты в их настоящем виде , осознавать , что лежит в их основе , кто является ключевыми игроками , и как с ними работать .',\n",
       "   'Midis gjithe problemeve me deficite ne luftojme me te sotmen — ne kryesisht mendojme per financa dhe ekonomi — per te cilat shqetsohemi me shume eshte deficiti i dialogu politike — aftesia jone per te adresuar konflikte moderne ashtu sic ato jane , për të shkuar tek burimi dhe per te kuptuar lojtaret kyç dhe te merremi me ta .',\n",
       "   'Günümüzde mücadele ettiğimiz bütün eksiklikler arasında — öncelikle finansal ve ekonomik olanları düşünüyoruz — beni en çok ilgilendireni politik diyalog eksikliği — modern çatışmaları oldukları haliyle irdeleme becerimiz , varoluş nedeninin kaynağına gitmek ve kilit oyuncuları anlamak ve onlarla anlaşmak .',\n",
       "   'Trong số tất cả những thâm hụt phiền phức mà chúng ta đang phải vật lộn để vượt qua — thường thì chúng ta chủ yếu nghĩ về tài chính và kinh tế — điều khiến tôi quan tâm nhất là sự thâm hụt trong đối thoại chính trị — khả năng chúng ta giải quyết các cuộc xung đột hiện đại đúng với bản chất của chúng , để đi tới nguồn gốc của vấn đề và để hiểu những nhân vật chủ chốt trong cuộc xung đột để có cách giải quyết với họ .',\n",
       "   '当今我们与之斗争的所有不足中 大家认为经济和金融缺陷是最紧要的 但最使我担忧的 是政治对话的不足 - 包括我们处理实际上的 现代冲突的能力 ， 追溯到这些冲突问题本源的能力 ， 以及理解关键人物 ， 并与他们沟通的能力 。',\n",
       "   '在所有今日世人仍然必需去努力實現的種種令人憂心的缺點之中 — - 我們認為金融和經濟是最根本主要的 — 但是我最關心的是 是政治上對話的不足 — 我們有能力應付現今的衝突 實際上就是 ， 找到事情發生的原因 了解關鍵人物 再和他們打交道 。']}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DatasetDict in module datasets.dataset_dict object:\n",
      "\n",
      "class DatasetDict(builtins.dict)\n",
      " |  A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DatasetDict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  cast_(self, features:datasets.features.Features)\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `feature` but :func:`cast_`\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          features (:class:`datasets.Features`): New features to cast the dataset to.\n",
      " |              The name and order of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n",
      " |  \n",
      " |  cleanup_cache_files(self) -> Dict[str, int]\n",
      " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n",
      " |      Be carefull when running this command that no other process is currently using other cache files.\n",
      " |      \n",
      " |      Return:\n",
      " |          Dict with the number of removed files for each split\n",
      " |  \n",
      " |  dictionary_encode_column_(self, column:str)\n",
      " |      Dictionary encode a column in each split.\n",
      " |      \n",
      " |          Dictionary encode can reduce the size of a column with many repetitions (e.g. string labels columns)\n",
      " |          by storing a dictionary of the strings. This only affect the internal storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |  \n",
      " |  filter(self, function, with_indices=False, input_columns:Union[str, List[str], NoneType]=None, batch_size:Union[int, NoneType]=1000, remove_columns:Union[List[str], NoneType]=None, keep_in_memory:bool=False, load_from_cache_file:bool=True, cache_file_names:Union[Dict[str, str], NoneType]=None, writer_batch_size:Union[int, NoneType]=1000, fn_kwargs:Union[dict, NoneType]=None, num_proc:Union[int, NoneType]=None) -> 'DatasetDict'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - `function(example: Dict) -> bool` if `with_indices=False`\n",
      " |              - `function(example: Dict, indices: int) -> bool` if `with_indices=True`\n",
      " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n",
      " |          num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |  \n",
      " |  flatten_(self, max_depth=16)\n",
      " |      Flatten the Apache Arrow Table of each split (nested features are flatten).\n",
      " |      Each column with a struct type is flattened into one column per struct field.\n",
      " |      Other columns are left unchanged.\n",
      " |  \n",
      " |  formatted_as(self, type:Union[str, NoneType]=None, columns:Union[List, NoneType]=None, output_all_columns:bool=False, **format_kwargs)\n",
      " |      To be used in a `with` statement. Set __getitem__ return format (type and columns)\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas']\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (Optional ``List[str]``): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  map(self, function, with_indices:bool=False, input_columns:Union[str, List[str], NoneType]=None, batched:bool=False, batch_size:Union[int, NoneType]=1000, remove_columns:Union[List[str], NoneType]=None, keep_in_memory:bool=False, load_from_cache_file:bool=True, cache_file_names:Union[Dict[str, str], NoneType]=None, writer_batch_size:Union[int, NoneType]=1000, features:Union[datasets.features.Features, NoneType]=None, disable_nullable:bool=False, fn_kwargs:Union[dict, NoneType]=None, num_proc:Union[int, NoneType]=None) -> 'DatasetDict'\n",
      " |      Apply a function to all the elements in the table (individually or in batches)\n",
      " |      and update the table (if function does updated examples).\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - `function(example: Dict) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n",
      " |              - `function(example: Dict, indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n",
      " |              - `function(batch: Dict[List]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n",
      " |              - `function(batch: Dict[List], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n",
      " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      " |          batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `True`): Disallow null values in the table.\n",
      " |          fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n",
      " |          num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |  \n",
      " |  remove_columns_(self, column_names:Union[str, List[str]])\n",
      " |      Remove one or several column(s) from each split in the dataset\n",
      " |      and the features associated to the column(s).\n",
      " |      \n",
      " |      The transformation is applied to all the splits of the dataset dictionary.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n",
      " |  \n",
      " |  rename_column_(self, original_column_name:str, new_column_name:str)\n",
      " |      Rename a column in the dataset and move the features associated to the original column under the new column name.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also rename a column using :func:`Dataset.map` with `remove_columns` but the present method:\n",
      " |          - takes care of moving the original features under the new column name.\n",
      " |          - doesn't copy the data to a new dataset and is thus much faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          original_column_name (:obj:`str`): Name of the column to rename.\n",
      " |          new_column_name (:obj:`str`): New name for the column.\n",
      " |  \n",
      " |  reset_format(self)\n",
      " |      Reset __getitem__ return format to python objects and all columns.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Same as ``self.set_format()``\n",
      " |  \n",
      " |  save_to_disk(self, dataset_dict_path:str)\n",
      " |      Save the dataset dict in a dataset dict directory.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_dict_path (``str``): path of the dataset dict directory where the dataset dict will be saved to\n",
      " |  \n",
      " |  set_format(self, type:Union[str, NoneType]=None, columns:Union[List, NoneType]=None, output_all_columns:bool=False, **format_kwargs)\n",
      " |      Set __getitem__ return format (type and columns)\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas']\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (Optional ``List[str]``): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  shuffle(self, seeds:Union[Dict[str, int], NoneType]=None, generators:Union[Dict[str, numpy.random._generator.Generator], NoneType]=None, keep_in_memory:bool=False, load_from_cache_file:bool=True, indices_cache_file_names:Union[Dict[str, str], NoneType]=None, writer_batch_size:Union[int, NoneType]=1000)\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |      \n",
      " |      Args:\n",
      " |          seeds (Optional `Dict[str, int]`): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |              You have to provide one :obj:`seed` per dataset in the dataset dictionary.\n",
      " |          generators (Optional `Dict[str, np.random.Generator]`): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |              You have to provide one :obj:`generator` per dataset in the dataset dictionary.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_names (`Optional[Dict[str, str]]`, default: `None`): Provide the name of a cache file to use to store the\n",
      " |              indices mappings instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |  \n",
      " |  sort(self, column:str, reverse:bool=False, kind:str=None, keep_in_memory:bool=False, load_from_cache_file:bool=True, indices_cache_file_names:Union[Dict[str, str], NoneType]=None, writer_batch_size:Union[int, NoneType]=1000) -> 'DatasetDict'\n",
      " |      Create a new dataset sorted according to a column.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Currently sorting according to a column name uses numpy sorting algorithm under the hood.\n",
      " |      The column should thus be a numpy compatible type (in particular not a nested type).\n",
      " |      This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`): column name to sort by.\n",
      " |          reverse: (`bool`, defaults to `False`): If True, sort by descending order rather then ascending.\n",
      " |          kind (Optional `str`): Numpy algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n",
      " |              The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n",
      " |              the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a cache file to use to store the\n",
      " |              indices mapping instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |  \n",
      " |  unique(self, column:str) -> Dict[str, List[Any]]\n",
      " |      Return a list of the unique elements in a column for each split.\n",
      " |      \n",
      " |      This is implemented in the low-level backend and as such, very fast.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              column name (list all the column names with :func:`datasets.Dataset.column_names`)\n",
      " |      \n",
      " |      Returns: Dict[:obj: `str`, :obj:`list`] of unique elements in the given column.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  load_from_disk(dataset_dict_path:str) -> 'DatasetDict'\n",
      " |      Load the dataset dict from a dataset dict directory\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_dict_path (``str``): path of the dataset dict directory where the dataset dict will be loaded from\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  cache_files\n",
      " |      The cache files containing the Apache Arrow table backing each split.\n",
      " |  \n",
      " |  column_names\n",
      " |      Names of the columns in each split of the dataset.\n",
      " |  \n",
      " |  data\n",
      " |      The Apache Arrow tables backing each split.\n",
      " |  \n",
      " |  num_columns\n",
      " |      Number of columns in each split of the dataset.\n",
      " |  \n",
      " |  num_rows\n",
      " |      Number of rows in each split of the dataset (same as :func:`datasets.Dataset.__len__`).\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of each split of the dataset (number of columns, number of rows).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
