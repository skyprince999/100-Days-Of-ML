{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-1.1.3-py3-none-any.whl (153 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (1.18.1)\n",
      "Requirement already satisfied: pandas in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (1.0.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (4.42.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2.22.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242 kB 25.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7 MB 53.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.25.10)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: dill, multiprocess, xxhash, pyarrow, datasets\n",
      "Successfully installed datasets-1.1.3 dill-0.3.3 multiprocess-0.70.11.1 pyarrow-2.0.0 xxhash-2.0.0\n"
     ]
    }
   ],
   "source": [
    "# install datasets\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n",
    "import pyarrow\n",
    "if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the library. We typically only need at most four methods:\n",
    "from datasets import list_datasets, list_metrics, load_dataset, load_metric\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤© Currently 316 datasets are available on the hub:\n",
      "['aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'ajgt_twitter_ar',\n",
      " 'allegro_reviews', 'allocine', 'amazon_reviews_multi', 'amazon_us_reviews',\n",
      " 'amttl', 'anli', 'arcd', 'arsentd_lev', 'art', 'aslg_pc12', 'asnq', 'asset',\n",
      " 'autshumato', 'big_patent', 'billsum', 'biomrc', 'blended_skill_talk', 'blimp',\n",
      " 'blog_authorship_corpus', 'bookcorpus', 'bookcorpusopen', 'boolq',\n",
      " 'break_data', 'c3', 'c4', 'cail2018', 'cawac', 'cdsc', 'cdt', 'cfq', 'chr_en',\n",
      " 'circa', 'civil_comments', 'clinc_oos', 'clue', 'cmrc2018', 'cnn_dailymail',\n",
      " 'coached_conv_pref', 'coarse_discourse', 'codah', 'com_qa', 'common_gen',\n",
      " 'commonsense_qa', 'compguesswhat', 'conll2000', 'conll2002', 'conll2003',\n",
      " 'conv_ai', 'coqa', 'cornell_movie_dialog', 'cos_e', 'cosmos_qa', 'crd3',\n",
      " 'crime_and_punish', 'crows_pairs', 'cs_restaurants', 'csv', 'daily_dialog',\n",
      " 'danish_political_comments', 'dart', 'dbpedia_14', 'deal_or_no_dialog',\n",
      " 'definite_pronoun_resolution', 'dialog_re', 'discofuse', 'docred', 'doqa',\n",
      " 'dream', 'drop', 'dyk', 'e2e_nlg', 'e2e_nlg_cleaned', 'eli5', 'emo', 'emotion',\n",
      " 'empathetic_dialogues', 'eraser_multi_rc', 'esnli', 'euronews', 'event2Mind',\n",
      " 'evidence_infer_treatment', 'exams', 'farsi_news', 'fever', 'flores', 'flue',\n",
      " 'fquad', 'gap', 'generated_reviews_enth', 'german_legal_entity_recognition',\n",
      " 'germaner', 'germeval_14', 'gigaword', 'glucose', 'glue', 'go_emotions',\n",
      " 'google_wellformed_query', 'great_code', 'guardian_authorship',\n",
      " 'gutenberg_time', 'hans', 'hansards', 'hard', 'health_fact',\n",
      " 'hebrew_sentiment', 'hellaswag', 'hotpot_qa', 'hyperpartisan_news_detection',\n",
      " 'imdb', 'indic_glue', 'inquisitive_qg', 'isixhosa_ner_corpus',\n",
      " 'isizulu_ner_corpus', 'iwslt2017', 'jeopardy', 'jfleg', 'json', 'kelm',\n",
      " 'kilt_tasks', 'kilt_wikipedia', 'kor_nli', 'kor_nlu', 'labr', 'lambada',\n",
      " 'lc_quad', 'librispeech_lm', 'lince', 'linnaeus', 'lm1b', 'math_dataset',\n",
      " 'math_qa', 'matinf', 'mc_taco', 'med_hop', 'medal', 'metooma', 'metrec',\n",
      " 'mlqa', 'mlsum', 'movie_rationales', 'mrqa', 'ms_marco', 'ms_terms',\n",
      " 'msra_ner', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'multi_woz_v22',\n",
      " 'multi_x_science_sum', 'mwsc', 'myanmar_news', 'natural_questions',\n",
      " 'ncbi_disease', 'nchlt', 'newsgroup', 'newsroom', 'nkjp-ner', 'nli_tr',\n",
      " 'norwegian_ner', 'numer_sense', 'openbookqa', 'openwebtext', 'opinosis',\n",
      " 'opus_dogc', 'opus_sardware', 'opus_xhosanavy', 'orange_sum', 'pandas',\n",
      " 'para_crawl', 'paws-x', 'pec', 'peoples_daily_ner', 'pg19', 'piaf', 'pib',\n",
      " 'piqa', 'polemo2', 'polyglot_ner', 'prachathai67k', 'pragmeval', 'qa4mre',\n",
      " 'qa_zre', 'qangaroo', 'qanta', 'qasc', 'qed', 'quac', 'quail', 'quarel',\n",
      " 'quartz', 'quora', 'quoref', 'race', 'reclor', 'reddit', 'reddit_tifu',\n",
      " 'reuters21578', 'ropes', 'rotten_tomatoes', 'scan', 'scb_mt_enth_2020',\n",
      " 'schema_guided_dstc8', 'scicite', 'scientific_papers', 'scifact', 'sciq',\n",
      " 'scitail', 'scitldr', 'search_qa', 'sem_eval_2010_task_8',\n",
      " 'sem_eval_2014_task_1', 'sentiment140', 'sepedi_ner', 'sesotho_ner_corpus',\n",
      " 'setswana_ner_corpus', 'sharc', 'sharc_modified', 'simple_questions_v2',\n",
      " 'siswati_ner_corpus', 'snli', 'social_bias_frames', 'social_i_qa',\n",
      " 'sogou_news', 'species_800', 'squad', 'squad_es', 'squad_it', 'squad_v1_pt',\n",
      " 'squad_v2', 'squadshifts', 'stsb_mt_sv', 'style_change_detection',\n",
      " 'super_glue', 'swag', 'swedish_ner_corpus', 'tamilmixsentiment', 'tashkeela',\n",
      " 'ted_hrlr', 'ted_multi', 'telugu_books', 'telugu_news', 'text', 'thaisum',\n",
      " 'tiny_shakespeare', 'tlc', 'totto', 'trec', 'trivia_qa', 'tsac', 'tunizi',\n",
      " 'tuple_ie', 'turku_ner_corpus', 'tweets_ar_en_parallel', 'tydiqa',\n",
      " 'ubuntu_dialogs_corpus', 'udhr', 'um005', 'universal_dependencies', 'web_nlg',\n",
      " 'web_of_science', 'web_questions', 'weibo_ner', 'wiki40b', 'wiki_auto',\n",
      " 'wiki_dpr', 'wiki_hop', 'wiki_qa', 'wiki_snippets', 'wiki_split', 'wikiann',\n",
      " 'wikihow', 'wikipedia', 'wikisql', 'wikitext', 'winogrande', 'wiqa',\n",
      " 'wisesight_sentiment', 'wmt14', 'wmt15', 'wmt16', 'wmt17', 'wmt18', 'wmt19',\n",
      " 'wmt_t2t', 'wnut_17', 'wongnai_reviews', 'woz_dialogue', 'x_stance', 'xcopa',\n",
      " 'xglue', 'xnli', 'xor_tydi_qa', 'xquad', 'xquad_r', 'xsum', 'xtreme',\n",
      " 'yahoo_answers_qa', 'yahoo_answers_topics', 'yelp_polarity', 'zest',\n",
      " 'Fraser/news-category-dataset', 'cdminix/mgb1', 'joelito/ler',\n",
      " 'joelito/sem_eval_2010_task_8', 'k-halid/ar', 'lhoestq/squad',\n",
      " 'mulcyber/europarl-mono', 'piEsposito/br-quad-2.0', 'piEsposito/br_quad_20',\n",
      " 'piEsposito/squad_20_ptbr', 'sshleifer/pseudo_bart_xsum']\n",
      "##################################################\n",
      "##################################################\n",
      "ðŸ¤© Currently 18 metrics are available on the hub:\n",
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'coval', 'f1', 'gleu', 'glue',\n",
      " 'indic_glue', 'meteor', 'precision', 'recall', 'rouge', 'sacrebleu', 'seqeval',\n",
      " 'squad', 'squad_v2', 'xnli']\n"
     ]
    }
   ],
   "source": [
    "# Currently available datasets and metrics\n",
    "datasets = list_datasets()\n",
    "metrics = list_metrics()\n",
    "\n",
    "print(f\"ðŸ¤© Currently {len(datasets)} datasets are available on the hub:\")\n",
    "pprint(datasets, compact=True)\n",
    "print(\"##################################################\")\n",
    "print(\"##################################################\")\n",
    "print(f\"ðŸ¤© Currently {len(metrics)} metrics are available on the hub:\")\n",
    "pprint(metrics, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': None,\n",
      " 'citation': '@article{2016arXiv160605250R,\\n'\n",
      "             '       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and '\n",
      "             '{Lopyrev},\\n'\n",
      "             '                 Konstantin and {Liang}, Percy},\\n'\n",
      "             '        title = \"{SQuAD: 100,000+ Questions for Machine '\n",
      "             'Comprehension of Text}\",\\n'\n",
      "             '      journal = {arXiv e-prints},\\n'\n",
      "             '         year = 2016,\\n'\n",
      "             '          eid = {arXiv:1606.05250},\\n'\n",
      "             '        pages = {arXiv:1606.05250},\\n'\n",
      "             'archivePrefix = {arXiv},\\n'\n",
      "             '       eprint = {1606.05250},\\n'\n",
      "             '}',\n",
      " 'description': 'Stanford Question Answering Dataset (SQuAD) is a reading '\n",
      "                'comprehension dataset, consisting of questions posed by '\n",
      "                'crowdworkers on a set of Wikipedia articles, where the answer '\n",
      "                'to every question is a segment of text, or span, from the '\n",
      "                'corresponding reading passage, or the question might be '\n",
      "                'unanswerable.',\n",
      " 'etag': None,\n",
      " 'id': 'squad',\n",
      " 'key': '',\n",
      " 'lastModified': None,\n",
      " 'siblings': None,\n",
      " 'size': None}\n"
     ]
    }
   ],
   "source": [
    "# You can access various attributes of the datasets before downloading them\n",
    "squad_dataset = list_datasets(with_details=True)[datasets.index('squad')]\n",
    "\n",
    "pprint(squad_dataset.__dict__)  # It's a simple python dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\n"
     ]
    }
   ],
   "source": [
    "# Downloading and loading a dataset\n",
    "dataset = load_dataset('squad', split='validation[:10%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'builder_name': 'squad',\n",
      " 'citation': '@article{2016arXiv160605250R,\\n'\n",
      "             '       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and '\n",
      "             '{Lopyrev},\\n'\n",
      "             '                 Konstantin and {Liang}, Percy},\\n'\n",
      "             '        title = \"{SQuAD: 100,000+ Questions for Machine '\n",
      "             'Comprehension of Text}\",\\n'\n",
      "             '      journal = {arXiv e-prints},\\n'\n",
      "             '         year = 2016,\\n'\n",
      "             '          eid = {arXiv:1606.05250},\\n'\n",
      "             '        pages = {arXiv:1606.05250},\\n'\n",
      "             'archivePrefix = {arXiv},\\n'\n",
      "             '       eprint = {1606.05250},\\n'\n",
      "             '}\\n',\n",
      " 'config_name': 'plain_text',\n",
      " 'dataset_size': 89789763,\n",
      " 'description': 'Stanford Question Answering Dataset (SQuAD) is a reading '\n",
      "                'comprehension dataset, consisting of questions posed by '\n",
      "                'crowdworkers on a set of Wikipedia articles, where the answer '\n",
      "                'to every question is a segment of text, or span, from the '\n",
      "                'corresponding reading passage, or the question might be '\n",
      "                'unanswerable.\\n',\n",
      " 'download_checksums': {'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json': {'checksum': '95aa6a52d5d6a735563366753ca50492a658031da74f301ac5238b03966972c9',\n",
      "                                                                                             'num_bytes': 4854279},\n",
      "                        'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json': {'checksum': '3527663986b8295af4f7fcdff1ba1ff3f72d07d61a20f487cb238a6ef92fd955',\n",
      "                                                                                               'num_bytes': 30288272}},\n",
      " 'download_size': 35142551,\n",
      " 'features': {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      "              'context': Value(dtype='string', id=None),\n",
      "              'id': Value(dtype='string', id=None),\n",
      "              'question': Value(dtype='string', id=None),\n",
      "              'title': Value(dtype='string', id=None)},\n",
      " 'homepage': 'https://rajpurkar.github.io/SQuAD-explorer/',\n",
      " 'license': '',\n",
      " 'post_processed': None,\n",
      " 'post_processing_size': None,\n",
      " 'size_in_bytes': 124932314,\n",
      " 'splits': {'train': SplitInfo(name='train', num_bytes=79317110, num_examples=87599, dataset_name='squad'),\n",
      "            'validation': SplitInfo(name='validation', num_bytes=10472653, num_examples=10570, dataset_name='squad')},\n",
      " 'supervised_keys': None,\n",
      " 'version': 1.0.0}\n"
     ]
    }
   ],
   "source": [
    "# Informations on the dataset (description, citation, size, splits, format...)\n",
    "# are provided in `dataset.info` (a simple python dataclass) and also as direct attributes in the dataset object\n",
    "pprint(dataset.info.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1057\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰Dataset len(dataset): 1057\n",
      "\n",
      "ðŸ‘‰First item 'dataset[0]':\n",
      "{'answers': {'answer_start': [177, 177, 177],\n",
      "             'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
      " 'context': 'Super Bowl 50 was an American football game to determine the '\n",
      "            'champion of the National Football League (NFL) for the 2015 '\n",
      "            'season. The American Football Conference (AFC) champion Denver '\n",
      "            'Broncos defeated the National Football Conference (NFC) champion '\n",
      "            'Carolina Panthers 24â€“10 to earn their third Super Bowl title. The '\n",
      "            \"game was played on February 7, 2016, at Levi's Stadium in the San \"\n",
      "            'Francisco Bay Area at Santa Clara, California. As this was the '\n",
      "            '50th Super Bowl, the league emphasized the \"golden anniversary\" '\n",
      "            'with various gold-themed initiatives, as well as temporarily '\n",
      "            'suspending the tradition of naming each Super Bowl game with '\n",
      "            'Roman numerals (under which the game would have been known as '\n",
      "            '\"Super Bowl L\"), so that the logo could prominently feature the '\n",
      "            'Arabic numerals 50.',\n",
      " 'id': '56be4db0acb8001400a502ec',\n",
      " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
      " 'title': 'Super_Bowl_50'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ‘‰Dataset len(dataset): {len(dataset)}\")\n",
    "print(\"\\nðŸ‘‰First item 'dataset[0]':\")\n",
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘‰Slice of the two items 'dataset[10:12]':\n",
      "OrderedDict([('answers',\n",
      "              [{'answer_start': [334, 334, 334],\n",
      "                'text': ['February 7, 2016', 'February 7', 'February 7, 2016']},\n",
      "               {'answer_start': [177, 177, 177],\n",
      "                'text': ['Denver Broncos',\n",
      "                         'Denver Broncos',\n",
      "                         'Denver Broncos']}]),\n",
      "             ('context',\n",
      "              ['Super Bowl 50 was an American football game to determine the '\n",
      "               'champion of the National Football League (NFL) for the 2015 '\n",
      "               'season. The American Football Conference (AFC) champion Denver '\n",
      "               'Broncos defeated the National Football Conference (NFC) '\n",
      "               'champion Carolina Panthers 24â€“10 to earn their third Super '\n",
      "               \"Bowl title. The game was played on February 7, 2016, at Levi's \"\n",
      "               'Stadium in the San Francisco Bay Area at Santa Clara, '\n",
      "               'California. As this was the 50th Super Bowl, the league '\n",
      "               'emphasized the \"golden anniversary\" with various gold-themed '\n",
      "               'initiatives, as well as temporarily suspending the tradition '\n",
      "               'of naming each Super Bowl game with Roman numerals (under '\n",
      "               'which the game would have been known as \"Super Bowl L\"), so '\n",
      "               'that the logo could prominently feature the Arabic numerals '\n",
      "               '50.',\n",
      "               'Super Bowl 50 was an American football game to determine the '\n",
      "               'champion of the National Football League (NFL) for the 2015 '\n",
      "               'season. The American Football Conference (AFC) champion Denver '\n",
      "               'Broncos defeated the National Football Conference (NFC) '\n",
      "               'champion Carolina Panthers 24â€“10 to earn their third Super '\n",
      "               \"Bowl title. The game was played on February 7, 2016, at Levi's \"\n",
      "               'Stadium in the San Francisco Bay Area at Santa Clara, '\n",
      "               'California. As this was the 50th Super Bowl, the league '\n",
      "               'emphasized the \"golden anniversary\" with various gold-themed '\n",
      "               'initiatives, as well as temporarily suspending the tradition '\n",
      "               'of naming each Super Bowl game with Roman numerals (under '\n",
      "               'which the game would have been known as \"Super Bowl L\"), so '\n",
      "               'that the logo could prominently feature the Arabic numerals '\n",
      "               '50.']),\n",
      "             ('id', ['56bea9923aeaaa14008c91bb', '56beace93aeaaa14008c91df']),\n",
      "             ('question',\n",
      "              ['What day was the Super Bowl played on?',\n",
      "               'Who won Super Bowl 50?']),\n",
      "             ('title', ['Super_Bowl_50', 'Super_Bowl_50'])])\n"
     ]
    }
   ],
   "source": [
    "# Or get slices with several examples:\n",
    "print(\"\\nðŸ‘‰Slice of the two items 'dataset[10:12]':\")\n",
    "pprint(dataset[10:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Which NFL team represented the AFC at Super Bowl 50?', 'Which NFL team represented the NFC at Super Bowl 50?', 'Where did Super Bowl 50 take place?', 'Which NFL team won Super Bowl 50?', 'What color was used to emphasize the 50th anniversary of the Super Bowl?', 'What was the theme of Super Bowl 50?', 'What day was the game played on?', 'What is the AFC short for?', 'What was the theme of Super Bowl 50?', 'What does AFC stand for?']\n"
     ]
    }
   ],
   "source": [
    "# You can get a full column of the dataset by indexing with its name as a string:\n",
    "print(dataset['question'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['question'] == dataset['question'][0])\n",
    "print(dataset[10:20]['context'] == dataset['context'][10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "['answers', 'context', 'id', 'question', 'title']\n",
      "Features:\n",
      "{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " 'context': Value(dtype='string', id=None),\n",
      " 'id': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None),\n",
      " 'title': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# You can inspect the dataset column names and types \n",
    "print(\"Column names:\")\n",
    "pprint(dataset.column_names)\n",
    "print(\"Features:\")\n",
    "pprint(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows 1057 also available as len(dataset) 1057\n",
      "The number of columns 5\n",
      "The shape (rows, columns) (1057, 5)\n"
     ]
    }
   ],
   "source": [
    "# Datasets also have shapes informations\n",
    "print(\"The number of rows\", dataset.num_rows, \"also available as len(dataset)\", len(dataset))\n",
    "print(\"The number of columns\", dataset.num_columns)\n",
    "print(\"The shape (rows, columns)\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775,"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de511ad8224368b642b2483efd6ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,179,179,179,179,179,179,179,179,179,179,179,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,704,704,704,704,704,704,704,704,704,704,704,704,704,704,353,353,353,353,353,353,353,353,353,353,353,353,353,353,353,464,464,464,464,464,464,464,464,464,464,464,464,464,464,464,464,306,306,306,306,306,306,306,306,306,306,306,306,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,496,496,496,496,496,496,496,496,496,496,496,496,496,496,496,260,260,260,260,260,260,260,260,260,874,874,874,874,874,874,874,874,874,874,874,874,874,874,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,176,176,176,176,176,176,176,176,176,176,176,176,176,176,176,176,782,782,782,782,782,782,782,782,782,782,782,782,782,782,782,782,536,536,536,536,536,536,536,536,536,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,495,495,495,495,495,495,495,495,495,495,495,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,441,441,441,441,441,441,441,441,441,441,441,357,357,357,357,357,357,357,357,357,296,296,296,296,296,296,296,296,296,296,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,804,804,804,804,804,804,804,804,804,804,804,397,397,397,397,397,397,397,397,397,397,397,397,397,397,360,360,360,360,360,360,360,973,973,973,973,973,973,973,973,973,973,973,973,973,973,263,263,263,263,263,263,263,263,263,263,263,568,568,568,568,568,568,568,568,568,568,568,264,264,264,264,264,264,264,264,264,264,264,264,264,264,264,892,892,892,892,892,892,892,892,892,892,892,206,206,206,206,206,489,489,489,489,489,489,489,489,489,489,489,489,489,181,181,181,181,181,181,181,181,181,181,181,181,531,531,531,531,531,531,531,531,531,531,531,531,664,664,664,664,664,664,664,664,664,664,664,664,664,664,672,672,672,672,672,672,672,672,672,672,672,672,672,672,858,858,858,858,858,858,858,858,858,858,858,858,634,634,634,634,634,634,634,634,634,634,634,634,634,634,891,891,891,891,891,891,891,891,891,891,891,891,891,488,488,488,488,488,488,488,488,488,488,488,488,942,942,942,942,942,942,942,942,942,942,942,942,942,942,942,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,522,522,522,522,522,1643,1643,1643,1643,1643,628,628,628,628,628,758,758,758,758,758,883,883,883,883,883,559,559,559,559,559,603,603,603,603,631,631,631,631,631,626,626,626,626,626,541,541,541,541,541,795,795,795,795,795,591,591,591,591,591,568,568,568,568,568,536,536,536,536,536,575,575,575,575,575,571,571,571,571,571,641,641,641,641,641,665,665,665,665,665,1088,1088,1088,1088,1088,1619,1619,1619,1619,1619,939,939,939,939,939,865,865,865,865,865,711,711,711,711,711,831,831,831,831,831,501,501,501,501,501,676,676,676,676,676,854,854,854,854,854,784,784,784,784,784,641,641,641,641,641,544,544,544,544,544,918,918,918,918,918,763,763,763,763,763,906,906,906,906,906,632,632,632,632,632,869,869,869,869,869,1044,1044,1044,1044,1044,760,760,760,760,760,715,715,715,715,715,838,838,838,838,838,881,881,881,881,881,940,940,940,940,940,618,618,618,618,618,1205,1205,1205,534,534,534,534,534,757,757,757,757,757,1239,1239,1239,1239,1239,609,609,609,609,609,798,798,798,798,798,613,613,613,613,613,613,613,613,613,613,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1057\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the length of each `context` string in our subset of the dataset\n",
    "# (10% of the validation i.e. 1057 examples)\n",
    "\n",
    "dataset.map(lambda example: print(len(example['context']), end=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in dataset:\n",
    "#     function(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775,"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a72f9c94b4d40248398bcbb9eac27c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,179,179,179,179,179,179,179,179,179,179,179,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,704,704,704,704,704,704,704,704,704,704,704,704,704,704,353,353,353,353,353,353,353,353,353,353,353,353,353,353,353,464,464,464,464,464,464,464,464,464,464,464,464,464,464,464,464,306,306,306,306,306,306,306,306,306,306,306,306,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,496,496,496,496,496,496,496,496,496,496,496,496,496,496,496,260,260,260,260,260,260,260,260,260,874,874,874,874,874,874,874,874,874,874,874,874,874,874,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,176,176,176,176,176,176,176,176,176,176,176,176,176,176,176,176,782,782,782,782,782,782,782,782,782,782,782,782,782,782,782,782,536,536,536,536,536,536,536,536,536,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,495,495,495,495,495,495,495,495,495,495,495,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,441,441,441,441,441,441,441,441,441,441,441,357,357,357,357,357,357,357,357,357,296,296,296,296,296,296,296,296,296,296,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,804,804,804,804,804,804,804,804,804,804,804,397,397,397,397,397,397,397,397,397,397,397,397,397,397,360,360,360,360,360,360,360,973,973,973,973,973,973,973,973,973,973,973,973,973,973,263,263,263,263,263,263,263,263,263,263,263,568,568,568,568,568,568,568,568,568,568,568,264,264,264,264,264,264,264,264,264,264,264,264,264,264,264,892,892,892,892,892,892,892,892,892,892,892,206,206,206,206,206,489,489,489,489,489,489,489,489,489,489,489,489,489,181,181,181,181,181,181,181,181,181,181,181,181,531,531,531,531,531,531,531,531,531,531,531,531,664,664,664,664,664,664,664,664,664,664,664,664,664,664,672,672,672,672,672,672,672,672,672,672,672,672,672,672,858,858,858,858,858,858,858,858,858,858,858,858,634,634,634,634,634,634,634,634,634,634,634,634,634,634,891,891,891,891,891,891,891,891,891,891,891,891,891,488,488,488,488,488,488,488,488,488,488,488,488,942,942,942,942,942,942,942,942,942,942,942,942,942,942,942,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,522,522,522,522,522,1643,1643,1643,1643,1643,628,628,628,628,628,758,758,758,758,758,883,883,883,883,883,559,559,559,559,559,603,603,603,603,631,631,631,631,631,626,626,626,626,626,541,541,541,541,541,795,795,795,795,795,591,591,591,591,591,568,568,568,568,568,536,536,536,536,536,575,575,575,575,575,571,571,571,571,571,641,641,641,641,641,665,665,665,665,665,1088,1088,1088,1088,1088,1619,1619,1619,1619,1619,939,939,939,939,939,865,865,865,865,865,711,711,711,711,711,831,831,831,831,831,501,501,501,501,501,676,676,676,676,676,854,854,854,854,854,784,784,784,784,784,641,641,641,641,641,544,544,544,544,544,918,918,918,918,918,763,763,763,763,763,906,906,906,906,906,632,632,632,632,632,869,869,869,869,869,1044,1044,1044,1044,1044,760,760,760,760,760,715,715,715,715,715,838,838,838,838,838,881,881,881,881,881,940,940,940,940,940,618,618,618,618,618,1205,1205,1205,534,534,534,534,534,757,757,757,757,757,1239,1239,1239,1239,1239,609,609,609,609,609,798,798,798,798,798,613,613,613,613,613,613,613,613,613,613,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1057\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import logging\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "dataset.map(lambda example: print(len(example['context']), end=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep it verbose for our tutorial though\n",
    "from datasets import logging\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-f5b2fe3c179dab04.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb9b18cfb984e71b308bf5c1aa9a58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1057 examples in 921948 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmphwnokt9r.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['My cute title: Super_Bowl_50', 'My cute title: Warsaw']\n"
     ]
    }
   ],
   "source": [
    "# Let's add a prefix 'My cute title: ' to each of our titles\n",
    "\n",
    "def add_prefix_to_title(example):\n",
    "    example['title'] = 'My cute title: ' + example['title']\n",
    "    return example\n",
    "\n",
    "prefixed_dataset = dataset.map(add_prefix_to_title)\n",
    "\n",
    "print(prefixed_dataset.unique('title'))  # `.unique()` is a super fast way to print the unique elemnts in a column (see the doc for all the methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-4e69a5d3bdc51b20.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f73be1833b4335b8747d7e2a9347a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1057 examples in 924062 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmplxeiofv2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['My cutest title: Super_Bowl_50', 'My cutest title: Warsaw']\n"
     ]
    }
   ],
   "source": [
    "# Since the input example dict is updated with our function output dict,\n",
    "# we can actually just return the updated 'title' field\n",
    "titled_dataset = dataset.map(lambda example: {'title': 'My cutest title: ' + example['title']})\n",
    "\n",
    "print(titled_dataset.unique('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-80781248ba7ff67f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8750932ace4badb8ab65272b7e9c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1057 examples in 915606 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmpv3el4_vz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['answers', 'context', 'id', 'new_title', 'question']\n",
      "['Wouhahh: Super_Bowl_50', 'Wouhahh: Warsaw']\n"
     ]
    }
   ],
   "source": [
    "# This will remove the 'title' column while doing the update (after having send it the the mapped function so you can use it in your function!)\n",
    "less_columns_dataset = dataset.map(lambda example: {'new_title': 'Wouhahh: ' + example['title']}, remove_columns=['title'])\n",
    "\n",
    "print(less_columns_dataset.column_names)\n",
    "print(less_columns_dataset.unique('new_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-b21142204e793319.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb172fffb2643bab356e755e18b92dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1057.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1057 examples in 911325 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmpbvp764lo.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['0: Which NFL team represented the AFC at Super Bowl 50?',\n",
      " '1: Which NFL team represented the NFC at Super Bowl 50?',\n",
      " '2: Where did Super Bowl 50 take place?',\n",
      " '3: Which NFL team won Super Bowl 50?',\n",
      " '4: What color was used to emphasize the 50th anniversary of the Super Bowl?']\n"
     ]
    }
   ],
   "source": [
    "# This will add the index in the dataset to the 'question' field\n",
    "with_indices_dataset = dataset.map(lambda example, idx: {'question': f'{idx}: ' + example['question']},\n",
    "                                   with_indices=True)\n",
    "\n",
    "pprint(with_indices_dataset['question'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.0.0-py3-none-any.whl (1.4 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 50.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: six in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.8)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=482540025452429d47a122f70e51ffafb84e59d1aac4867a76664e5c54734c59\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sacremoses, tokenizers, transformers\n",
      "Successfully installed regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import a fast tokenizer that can work on batched inputs\n",
    "# (the 'Fast' tokenizers in HuggingFace)\n",
    "from transformers import BertTokenizerFast, logging as transformers_logging\n",
    "\n",
    "transformers_logging.set_verbosity_warning()\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-345aae1e66add54d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e5babcb078440a85ac152bd79a429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1057 examples in 4714929 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmp4g1izzjt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "encoded_dataset[0]\n",
      "{'answers': {'answer_start': [177, 177, 177],\n",
      "             'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1],\n",
      " 'context': 'Super Bowl 50 was an American football game to determine the '\n",
      "            'champion of the National Football League (NFL) for the 2015 '\n",
      "            'season. The American Football Conference (AFC) champion Denver '\n",
      "            'Broncos defeated the National Football Conference (NFC) champion '\n",
      "            'Carolina Panthers 24â€“10 to earn their third Super Bowl title. The '\n",
      "            \"game was played on February 7, 2016, at Levi's Stadium in the San \"\n",
      "            'Francisco Bay Area at Santa Clara, California. As this was the '\n",
      "            '50th Super Bowl, the league emphasized the \"golden anniversary\" '\n",
      "            'with various gold-themed initiatives, as well as temporarily '\n",
      "            'suspending the tradition of naming each Super Bowl game with '\n",
      "            'Roman numerals (under which the game would have been known as '\n",
      "            '\"Super Bowl L\"), so that the logo could prominently feature the '\n",
      "            'Arabic numerals 50.',\n",
      " 'id': '56be4db0acb8001400a502ec',\n",
      " 'input_ids': [101, 3198, 5308, 1851, 1108, 1126, 1237, 1709, 1342, 1106, 4959,\n",
      "               1103, 3628, 1104, 1103, 1305, 2289, 1453, 113, 4279, 114, 1111,\n",
      "               1103, 1410, 1265, 119, 1109, 1237, 2289, 3047, 113, 10402, 114,\n",
      "               3628, 7068, 14722, 2378, 1103, 1305, 2289, 3047, 113, 24743, 114,\n",
      "               3628, 2938, 13598, 1572, 782, 1275, 1106, 7379, 1147, 1503, 3198,\n",
      "               5308, 1641, 119, 1109, 1342, 1108, 1307, 1113, 1428, 128, 117,\n",
      "               1446, 117, 1120, 12388, 112, 188, 3339, 1107, 1103, 1727, 2948,\n",
      "               2410, 3894, 1120, 3364, 10200, 117, 1756, 119, 1249, 1142, 1108,\n",
      "               1103, 13163, 3198, 5308, 117, 1103, 2074, 13463, 1103, 107, 5404,\n",
      "               5453, 107, 1114, 1672, 2284, 118, 12005, 11751, 117, 1112, 1218,\n",
      "               1112, 7818, 28117, 20080, 16264, 1103, 3904, 1104, 10505, 1296,\n",
      "               3198, 5308, 1342, 1114, 2264, 183, 15447, 16179, 113, 1223, 1134,\n",
      "               1103, 1342, 1156, 1138, 1151, 1227, 1112, 107, 3198, 5308, 149,\n",
      "               107, 114, 117, 1177, 1115, 1103, 7998, 1180, 15199, 2672, 1103,\n",
      "               4944, 183, 15447, 16179, 1851, 119, 102],\n",
      " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
      " 'title': 'Super_Bowl_50',\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0]}\n"
     ]
    }
   ],
   "source": [
    "# Now let's batch tokenize our dataset 'context'\n",
    "encoded_dataset = dataset.map(lambda example: tokenizer(example['context']), batched=True)\n",
    "\n",
    "print(\"encoded_dataset[0]\")\n",
    "pprint(encoded_dataset[0], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['answers', 'context', 'id', 'question', 'title']\n"
     ]
    }
   ],
   "source": [
    "# we have added additional columns\n",
    "pprint(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-391a337dc71b242a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63dc050adf8431cad92ffd15204066d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1057 examples in 5081257 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmpcvg3ygx5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let show a more complex processing with the full preparation of the SQuAD dataset\n",
    "# for training a model from Transformers\n",
    "def convert_to_features(batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    input_pairs = list(zip())\n",
    "    encodings = tokenizer(batch['context'], batch['question'], truncation=True)\n",
    "\n",
    "    # Compute start and end tokens for labels\n",
    "    start_positions, end_positions = [], []\n",
    "    for i, answer in enumerate(batch['answers']):\n",
    "        first_char = answer['answer_start'][0]\n",
    "        last_char = first_char + len(answer['text'][0]) - 1\n",
    "        start_positions.append(encodings.char_to_token(i, first_char))\n",
    "        end_positions.append(encodings.char_to_token(i, last_char))\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "encoded_dataset = dataset.map(convert_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names ['answers', 'attention_mask', 'context', 'end_positions', 'id', 'input_ids', 'question', 'start_positions', 'title', 'token_type_ids']\n",
      "start_positions [34, 45, 80, 34, 98]\n"
     ]
    }
   ],
   "source": [
    "# Now our dataset comprise the labels for the start and end position\n",
    "# as well as the offsets for converting back tokens\n",
    "# in span of the original string for evaluation\n",
    "print(\"column_names\", encoded_dataset.column_names)\n",
    "print(\"start_positions\", encoded_dataset[:5]['start_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set __getitem__(key) output type to torch for ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]),\n",
      " 'end_positions': tensor(46),\n",
      " 'input_ids': tensor([  101,  3198,  5308,  1851,  1108,  1126,  1237,  1709,  1342,  1106,\n",
      "         4959,  1103,  3628,  1104,  1103,  1305,  2289,  1453,   113,  4279,\n",
      "          114,  1111,  1103,  1410,  1265,   119,  1109,  1237,  2289,  3047,\n",
      "          113, 10402,   114,  3628,  7068, 14722,  2378,  1103,  1305,  2289,\n",
      "         3047,   113, 24743,   114,  3628,  2938, 13598,  1572,   782,  1275,\n",
      "         1106,  7379,  1147,  1503,  3198,  5308,  1641,   119,  1109,  1342,\n",
      "         1108,  1307,  1113,  1428,   128,   117,  1446,   117,  1120, 12388,\n",
      "          112,   188,  3339,  1107,  1103,  1727,  2948,  2410,  3894,  1120,\n",
      "         3364, 10200,   117,  1756,   119,  1249,  1142,  1108,  1103, 13163,\n",
      "         3198,  5308,   117,  1103,  2074, 13463,  1103,   107,  5404,  5453,\n",
      "          107,  1114,  1672,  2284,   118, 12005, 11751,   117,  1112,  1218,\n",
      "         1112,  7818, 28117, 20080, 16264,  1103,  3904,  1104, 10505,  1296,\n",
      "         3198,  5308,  1342,  1114,  2264,   183, 15447, 16179,   113,  1223,\n",
      "         1134,  1103,  1342,  1156,  1138,  1151,  1227,  1112,   107,  3198,\n",
      "         5308,   149,   107,   114,   117,  1177,  1115,  1103,  7998,  1180,\n",
      "        15199,  2672,  1103,  4944,   183, 15447, 16179,  1851,   119,   102,\n",
      "         5979,  4279,  1264,  2533,  1103, 24743,  1120,  3198,  5308,  1851,\n",
      "          136,   102]),\n",
      " 'start_positions': tensor(45),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "columns_to_return = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "\n",
    "encoded_dataset.set_format(type='torch', columns=columns_to_return)\n",
    "\n",
    "# Our dataset indexing output is now ready for being used in a pytorch dataloader\n",
    "pprint(encoded_dataset[1], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['answers', 'attention_mask', 'context', 'end_positions', 'id', 'input_ids', 'question', 'start_positions', 'title', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "# Note that the columns are not removed from the dataset, just not returned when calling __getitem__\n",
    "# Similarly the inner type of the dataset is not changed to torch.Tensor, the conversion and filtering is done on-the-fly when querying the dataset\n",
    "print(encoded_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [249, 249, 249],\n",
      "             'text': ['Carolina Panthers', 'Carolina Panthers',\n",
      "                      'Carolina Panthers']},\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'context': 'Super Bowl 50 was an American football game to determine the '\n",
      "            'champion of the National Football League (NFL) for the 2015 '\n",
      "            'season. The American Football Conference (AFC) champion Denver '\n",
      "            'Broncos defeated the National Football Conference (NFC) champion '\n",
      "            'Carolina Panthers 24â€“10 to earn their third Super Bowl title. The '\n",
      "            \"game was played on February 7, 2016, at Levi's Stadium in the San \"\n",
      "            'Francisco Bay Area at Santa Clara, California. As this was the '\n",
      "            '50th Super Bowl, the league emphasized the \"golden anniversary\" '\n",
      "            'with various gold-themed initiatives, as well as temporarily '\n",
      "            'suspending the tradition of naming each Super Bowl game with '\n",
      "            'Roman numerals (under which the game would have been known as '\n",
      "            '\"Super Bowl L\"), so that the logo could prominently feature the '\n",
      "            'Arabic numerals 50.',\n",
      " 'end_positions': 46,\n",
      " 'id': '56be4db0acb8001400a502ed',\n",
      " 'input_ids': [101, 3198, 5308, 1851, 1108, 1126, 1237, 1709, 1342, 1106, 4959,\n",
      "               1103, 3628, 1104, 1103, 1305, 2289, 1453, 113, 4279, 114, 1111,\n",
      "               1103, 1410, 1265, 119, 1109, 1237, 2289, 3047, 113, 10402, 114,\n",
      "               3628, 7068, 14722, 2378, 1103, 1305, 2289, 3047, 113, 24743, 114,\n",
      "               3628, 2938, 13598, 1572, 782, 1275, 1106, 7379, 1147, 1503, 3198,\n",
      "               5308, 1641, 119, 1109, 1342, 1108, 1307, 1113, 1428, 128, 117,\n",
      "               1446, 117, 1120, 12388, 112, 188, 3339, 1107, 1103, 1727, 2948,\n",
      "               2410, 3894, 1120, 3364, 10200, 117, 1756, 119, 1249, 1142, 1108,\n",
      "               1103, 13163, 3198, 5308, 117, 1103, 2074, 13463, 1103, 107, 5404,\n",
      "               5453, 107, 1114, 1672, 2284, 118, 12005, 11751, 117, 1112, 1218,\n",
      "               1112, 7818, 28117, 20080, 16264, 1103, 3904, 1104, 10505, 1296,\n",
      "               3198, 5308, 1342, 1114, 2264, 183, 15447, 16179, 113, 1223, 1134,\n",
      "               1103, 1342, 1156, 1138, 1151, 1227, 1112, 107, 3198, 5308, 149,\n",
      "               107, 114, 117, 1177, 1115, 1103, 7998, 1180, 15199, 2672, 1103,\n",
      "               4944, 183, 15447, 16179, 1851, 119, 102, 5979, 4279, 1264, 2533,\n",
      "               1103, 24743, 1120, 3198, 5308, 1851, 136, 102],\n",
      " 'question': 'Which NFL team represented the NFC at Super Bowl 50?',\n",
      " 'start_positions': 45,\n",
      " 'title': 'Super_Bowl_50',\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# We can remove the formatting with `.reset_format()`\n",
    "# or, identically, a call to `.set_format()` with no arguments\n",
    "encoded_dataset.reset_format()\n",
    "\n",
    "pprint(encoded_dataset[1], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columns': ['answers',\n",
      "             'attention_mask',\n",
      "             'context',\n",
      "             'end_positions',\n",
      "             'id',\n",
      "             'input_ids',\n",
      "             'question',\n",
      "             'start_positions',\n",
      "             'title',\n",
      "             'token_type_ids'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': None}\n"
     ]
    }
   ],
   "source": [
    "# The current format can be checked with `.format`,\n",
    "# which is a dict of the type and formatting\n",
    "pprint(encoded_dataset.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.0.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: requests in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: sacremoses in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: six in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /home/ubuntu/.cache/huggingface/datasets/e0bdcacbb45db988fbbf4f0e0974c8cb2bf0281198e77c267a9ae22d1214616a.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py for additional imports.\n",
      "Lock 139703862266232 acquired on /home/ubuntu/.cache/huggingface/datasets/e0bdcacbb45db988fbbf4f0e0974c8cb2bf0281198e77c267a9ae22d1214616a.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py.lock\n",
      "Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/squad/squad.py at /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/squad\n",
      "Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/squad/squad.py at /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/squad/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\n",
      "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/squad/squad.py to /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/squad/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/squad.py\n",
      "Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/squad/dataset_infos.json to /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/squad/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/dataset_infos.json\n",
      "Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/squad/squad.py at /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/squad/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/squad.json\n",
      "Lock 139703862266232 released on /home/ubuntu/.cache/huggingface/datasets/e0bdcacbb45db988fbbf4f0e0974c8cb2bf0281198e77c267a9ae22d1214616a.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py.lock\n",
      "No config specified, defaulting to first: squad/plain_text\n",
      "Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/squad/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\n",
      "Lock 139703862265504 acquired on /home/ubuntu/.cache/huggingface/datasets/_home_ubuntu_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\n",
      "Overwrite dataset info from restored data version.\n",
      "Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\n",
      "Lock 139703862265504 released on /home/ubuntu/.cache/huggingface/datasets/_home_ubuntu_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\n",
      "Lock 139703862267128 acquired on /home/ubuntu/.cache/huggingface/datasets/_home_ubuntu_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\n",
      "Reusing dataset squad (/home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\n",
      "Lock 139703862267128 released on /home/ubuntu/.cache/huggingface/datasets/_home_ubuntu_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\n",
      "Constructing Dataset for split train, validation, from /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 92.63it/s]\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-fbbea265347983d0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef3c41010a04dfaa21c59014f101be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 87599 examples in 452536620 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmpkqecat4h.\n",
      "Testing the mapped function outputs\n",
      "Testing finished, running the mapping function on the dataset\n",
      "Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-035645402298610f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ab4c686cbb4d9bb049dd0cbe0e0e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 10570 examples in 56664663 bytes /home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/tmpkopprowq.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
      "Set __getitem__(key) output type to torch for ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load our training dataset and tokenizer\n",
    "dataset = load_dataset('squad')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    gold_text = answer['text'][0]\n",
    "    start_idx = answer['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        return start_idx, end_idx       # When the gold label position is good\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "        return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "        return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example_batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer(example_batch['context'], example_batch['question'], truncation=True)\n",
    "\n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methods.\n",
    "    start_positions, end_positions = [], []\n",
    "    for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\n",
    "        start_idx, end_idx = get_correct_alignement(context, answer)\n",
    "        start_positions.append(encodings.char_to_token(i, start_idx))\n",
    "        end_positions.append(encodings.char_to_token(i, end_idx-1))\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "encoded_dataset = dataset.map(convert_to_features, batched=True)\n",
    "\n",
    "# Format our dataset to outputs torch.Tensor to train a pytorch model\n",
    "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "encoded_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "# Instantiate a PyTorch Dataloader around our dataset\n",
    "# Let's do dynamic batching (pad on the fly with our own collate_fn)\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, return_tensors='pt')\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset['train'], collate_fn=collate_fn, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0676a5e1067d4863968f66a4a6d025a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47b4cdaca8c4ea3899792e446f046bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForQuestionAnswering: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Let's load a pretrained Bert model and a simple optimizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('distilbert-base-cased', return_dict=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's train our model\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model.train().to(device)\n",
    "# for i, batch in enumerate(dataloader):\n",
    "#     batch.to(device)\n",
    "#     outputs = model(**batch)\n",
    "#     loss = outputs.loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     model.zero_grad()\n",
    "#     print(f'Step {i} - loss: {loss:.3}')\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lock 139703876655032 acquired on /home/ubuntu/.cache/huggingface/datasets/f6b8871d16bf6d7f7f9f0d1e942f224dd6b8e3806e66caca3d8292a34a3f3b7e.90615c613092df9760941da0e77d044445c7ed8d9bf8d99e9319993d91df4055.py.lock\n",
      "https://raw.githubusercontent.com/huggingface/datasets/1.1.3/metrics/sacrebleu/sacrebleu.py not found in cache or force_download set to True, downloading to /home/ubuntu/.cache/huggingface/datasets/tmpkrnr_6se\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e29c3541e294e52b09dbbe17e079cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1880.0, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://raw.githubusercontent.com/huggingface/datasets/1.1.3/metrics/sacrebleu/sacrebleu.py in cache at /home/ubuntu/.cache/huggingface/datasets/f6b8871d16bf6d7f7f9f0d1e942f224dd6b8e3806e66caca3d8292a34a3f3b7e.90615c613092df9760941da0e77d044445c7ed8d9bf8d99e9319993d91df4055.py\n",
      "creating metadata file for /home/ubuntu/.cache/huggingface/datasets/f6b8871d16bf6d7f7f9f0d1e942f224dd6b8e3806e66caca3d8292a34a3f3b7e.90615c613092df9760941da0e77d044445c7ed8d9bf8d99e9319993d91df4055.py\n",
      "Lock 139703876655032 released on /home/ubuntu/.cache/huggingface/datasets/f6b8871d16bf6d7f7f9f0d1e942f224dd6b8e3806e66caca3d8292a34a3f3b7e.90615c613092df9760941da0e77d044445c7ed8d9bf8d99e9319993d91df4055.py.lock\n",
      "Checking /home/ubuntu/.cache/huggingface/datasets/f6b8871d16bf6d7f7f9f0d1e942f224dd6b8e3806e66caca3d8292a34a3f3b7e.90615c613092df9760941da0e77d044445c7ed8d9bf8d99e9319993d91df4055.py for additional imports.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use this metric, you need to install the following dependencies['sacrebleu'] using 'pip install sacrebleu' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a5ee50e4934b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msacrebleu_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sacrebleu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# If you only have a single iteration, you can easily compute the score like this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_metric\u001b[0;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, script_version, **metric_init_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m     \u001b[0mmetric_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_main_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mprepare_module\u001b[0;34m(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneeds_to_be_installed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         raise ImportError(\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;34mf\"To be able to use this {module_type}, you need to install the following dependencies\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;34mf\"{[lib_name for lib_name, lib_path in needs_to_be_installed]} using 'pip install \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;34mf\"{' '.join([lib_path for lib_name, lib_path in needs_to_be_installed])}' for instance'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: To be able to use this metric, you need to install the following dependencies['sacrebleu'] using 'pip install sacrebleu' for instance'"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "sacrebleu_metric = load_metric('sacrebleu')\n",
    " \n",
    "# If you only have a single iteration, you can easily compute the score like this\n",
    "predictions = model(inputs)\n",
    "score = sacrebleu_metric.compute(predictions, references)\n",
    " \n",
    "# If you have a loop, you can \"add\" your predictions and references at each iteration instead of having to save them yourself (the metric object store them efficiently for you)\n",
    "for batch in dataloader:\n",
    "    model_input, targets = batch\n",
    "    predictions = model(model_inputs)\n",
    "    sacrebleu_metric.add_batch(predictions, targets)\n",
    "score = sacrebleu_metric.compute()  # Compute the score from all the stored predictions/references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
