{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import absl\n",
    "import datetime\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.lib.io.file_io import recursive_create_dir\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NQ_DIR = \"nq-competition\"\n",
    "# NQ_DIR contains some packages / modules\n",
    "\n",
    "sys.path.append(NQ_DIR)\n",
    "sys.path.append(os.path.join(NQ_DIR, \"transformers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamw_optimizer import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------------------------------------------------------------\n",
    "# Ref: https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/custom_training.ipynb#scrollTo=jwJtsCQhHK-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your TPU node internal ip\n",
    "TPU_WORKER = 'grpc://10.240.1.2:8470'\n",
    "\n",
    "# Your TPU Zone, for example 'europe-west4-a'\n",
    "ZONE = 'europe-west4-a'\n",
    "\n",
    "# Your project name, for example, 'kaggle-nq-123456'\n",
    "PROJECT = 'project-x-262017'\n",
    "\n",
    "# Your training tf record file on Google Storage bucket. For example, gs://kaggle-my-nq-competition/nq_train.tfrecord\n",
    "TRAIN_TF_RECORD = '/home/jupyter/bert-joint-baseline/nq-train.tfrecords-00000-of-00001'\n",
    "\n",
    "# Your checkpoint dir on Google Storage bucket. For example, \"gs://kaggle-my-nq-competition/checkpoints/\"\n",
    "CHECKPOINT_DIR = '/home/jupyter/checkpoints/distilbert-base-uncased-distilled-squad/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = True\n",
    "INPUT_DIR = \"/kaggle/input/\"\n",
    "\n",
    "# The original Bert Joint Baseline data.\n",
    "BERT_JOINT_BASE_DIR = os.path.join(INPUT_DIR, \"bertjointbaseline\")\n",
    "\n",
    "# This nq dir contains all files for publicly use.\n",
    "NQ_DIR = os.path.join(INPUT_DIR, \"nq-competition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_OWN_NQ_DIR = NQ_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local usage.\n",
    "if not os.path.isdir(INPUT_DIR):\n",
    "    IS_KAGGLE = False\n",
    "    INPUT_DIR = \"./\"\n",
    "    NQ_DIR = \"./\"\n",
    "    MY_OWN_NQ_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NQ_DIR contains some packages / modules\n",
    "sys.path.append(NQ_DIR)\n",
    "sys.path.append(os.path.join(NQ_DIR, \"transformers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nq_flags import DEFAULT_FLAGS as FLAGS\n",
    "from nq_flags import del_all_flags\n",
    "from nq_dataset_utils import *\n",
    "\n",
    "import sacremoses as sm\n",
    "import transformers\n",
    "from adamw_optimizer import AdamW\n",
    "\n",
    "from transformers import TFBertModel\n",
    "from transformers import TFBertMainLayer, TFBertPreTrainedModel\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel, TFDistilBertModel\n",
    "from transformers import TFBertMainLayer, TFDistilBertMainLayer, TFBertPreTrainedModel, TFDistilBertPreTrainedModel\n",
    "from transformers.modeling_tf_utils import get_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODELS = {\n",
    "    \"BERT\": [\n",
    "        'bert-base-uncased',\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "    ],\n",
    "    \"DISTILBERT\": [\n",
    "        'distilbert-base-uncased-distilled-squad'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = absl.flags\n",
    "del_all_flags(flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "vocab_file = os.path.join(NQ_DIR, \"vocab-nq.txt\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", vocab_file,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length_for_training\", 512,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization for training examples. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 512,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns_for_training\", 0.02,\n",
    "    \"If positive, for converting training dataset, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", -1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "flags.DEFINE_string(\"train_tf_record\", TRAIN_TF_RECORD,\n",
    "                    \"Precomputed tf records for training dataset.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training dataset.\")\n",
    "\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"input_checkpoint_dir\", CHECKPOINT_DIR,\n",
    "    \"The root directory that contains checkpoints to be loaded of all trained models.\")\n",
    "\n",
    "flags.DEFINE_string(\"model_dir\", NQ_DIR, \"Root dir of all Hugging Face's models\")\n",
    "\n",
    "flags.DEFINE_string(\"model_name\", \"bert-large-uncased-whole-word-masking-finetuned-squad\", \"Name of Hugging Face's model to use.\")\n",
    "\n",
    "flags.DEFINE_integer(\"epochs\", 2, \"Total epochs for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 64 * 8, \"Batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"shuffle_buffer_size\", 100000, \"Shuffle buffer size for training.\")\n",
    "\n",
    "flags.DEFINE_float(\"init_learning_rate\", 5e-5, \"The initial learning rate for AdamW optimizer.\")\n",
    "\n",
    "flags.DEFINE_bool(\"cyclic_learning_rate\", True, \"If to use cyclic learning rate.\")\n",
    "\n",
    "flags.DEFINE_float(\"init_weight_decay_rate\", 0.01, \"The initial weight decay rate for AdamW optimizer.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_warmup_steps\", 0, \"Number of training steps to perform linear learning rate warmup.\")\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_examples\", None, \"Number of precomputed training steps in 1 epoch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the default flags as parsed flags\n",
    "FLAGS.mark_as_parsed()\n",
    "\n",
    "NB_SHORT_ANSWER_TYPES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tf_record_file, seq_length, batch_size=1, shuffle_buffer_size=0, is_training=False):\n",
    "\n",
    "    if is_training:\n",
    "        features = {\n",
    "            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"start_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"end_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"answer_types\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    else:\n",
    "        features = {\n",
    "            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64)\n",
    "        }        \n",
    "\n",
    "    def decode_record(record, features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.io.parse_single_example(record, features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "    def select_data_from_record(record):\n",
    "        \n",
    "        x = {\n",
    "            'unique_ids': record['unique_ids'],\n",
    "            'input_ids': record['input_ids'],\n",
    "            'input_mask': record['input_mask'],\n",
    "            'segment_ids': record['segment_ids']\n",
    "        }\n",
    "\n",
    "        if is_training:\n",
    "            y = {\n",
    "                'short_start_positions': record['start_positions'],\n",
    "                'short_end_positions': record['end_positions'],\n",
    "                'short_answer_types': record['answer_types']\n",
    "            }\n",
    "\n",
    "            return (x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_file)\n",
    "    \n",
    "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
    "    dataset = dataset.map(select_data_from_record)\n",
    "    \n",
    "    if shuffle_buffer_size > 0:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_connect_to_cluster(cluster_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.240.1.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.240.1.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.tpu.topology.Topology at 0x7fbda029da20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11021447881587618788)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11021447881587618788)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 181624137605752507)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 181624137605752507)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2940472611495834783)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2940472611495834783)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7269179663733584927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7269179663733584927)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8099349451272825642)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8099349451272825642)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16072348469113498217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16072348469113498217)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11134326371993417661)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11134326371993417661)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10559094424008662903)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10559094424008662903)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11598980095336142458)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11598980095336142458)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 15046180983821013037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 15046180983821013037)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17463006931088368389)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17463006931088368389)\n"
     ]
    }
   ],
   "source": [
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.num_train_examples is None:\n",
    "    FLAGS.num_train_examples = 494670\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNQModel:\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Subclasses of this class are different in self.backend,\n",
    "        which should be a model that outputs a tensor of shape (batch_size, hidden_dim), and the\n",
    "        `backend_call()` method.\n",
    "        \n",
    "        We will use Hugging Face Bert/DistilBert as backend in this notebook.\n",
    "        \"\"\"\n",
    "\n",
    "        self.backend = None\n",
    "        \n",
    "        self.seq_output_dropout = tf.keras.layers.Dropout(kwargs.get('seq_output_dropout_prob', 0.05))\n",
    "        self.pooled_output_dropout = tf.keras.layers.Dropout(kwargs.get('pooled_output_dropout_prob', 0.05))\n",
    "    \n",
    "        self.short_pos_classifier = tf.keras.layers.Dense(2,\n",
    "                                        kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                        name='pos_classifier')       \n",
    "\n",
    "        self.short_answer_type_classifier = tf.keras.layers.Dense(NB_SHORT_ANSWER_TYPES,\n",
    "                                        kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                        name='answer_type_classifier')        \n",
    "                \n",
    "    def backend_call(self, inputs, **kwargs):\n",
    "        \"\"\"This method should be implemented by subclasses.\n",
    "           \n",
    "           The implementation should take into account the (somehow) different input formats of Hugging Face's\n",
    "           models.\n",
    "           \n",
    "           For example, the `TFDistilBert` model, unlike `Bert` model, doesn't have segment_id as input.\n",
    "           \n",
    "           Then it calls `self.backend_call()` to get the outputs from Bert's model, which is used in self.call().\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \n",
    "        # sequence / [CLS] outputs from original bert\n",
    "        sequence_output, pooled_output = self.backend_call(inputs, **kwargs)  # shape = (batch_size, seq_len, hidden_dim) / (batch_size, hidden_dim)\n",
    "        \n",
    "        # dropout\n",
    "        sequence_output = self.seq_output_dropout(sequence_output, training=kwargs.get('training', False))\n",
    "        pooled_output = self.pooled_output_dropout(pooled_output, training=kwargs.get('training', False))\n",
    "    \n",
    "        short_pos_logits = self.short_pos_classifier(sequence_output)  # shape = (batch_size, seq_len, 2)\n",
    "        \n",
    "        short_start_pos_logits = short_pos_logits[:, :, 0]  # shape = (batch_size, seq_len)\n",
    "        short_end_pos_logits = short_pos_logits[:, :, 1]  # shape = (batch_size, seq_len)\n",
    "        \n",
    "        short_answer_type_logits = self.short_answer_type_classifier(pooled_output)  # shape = (batch_size, NB_SHORT_ANSWER_TYPES)\n",
    "\n",
    "        outputs = (short_start_pos_logits, short_end_pos_logits, short_answer_type_logits)\n",
    "\n",
    "        return outputs  # logits\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForNQ(TFNQModel, TFBertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        \n",
    "        TFBertPreTrainedModel.__init__(self, config, *inputs, **kwargs)  # explicit calls without super\n",
    "        TFNQModel.__init__(self, config)\n",
    "\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "    \n",
    "    def backend_call(self, inputs, **kwargs):\n",
    "        \n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        sequence_output, pooled_output = outputs[0], outputs[1]  # shape = (batch_size, seq_len, hidden_dim) / (batch_size, hidden_dim)\n",
    "        \n",
    "        return sequence_output, pooled_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDistilBertForNQ(TFNQModel, TFDistilBertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        \n",
    "        TFDistilBertPreTrainedModel.__init__(self, config, *inputs, **kwargs)  # explicit calls without super\n",
    "        TFNQModel.__init__(self, config)\n",
    "\n",
    "        self.backend = TFDistilBertMainLayer(config, name=\"distilbert\")\n",
    "        \n",
    "    def backend_call(self, inputs, **kwargs):\n",
    "        \n",
    "        if isinstance(inputs, tuple):\n",
    "            # Distil bert has no segment_id (i.e. `token_type_ids`)\n",
    "            inputs = inputs[:2]\n",
    "        else:\n",
    "            inputs = inputs\n",
    "        \n",
    "        outputs = self.backend(inputs, **kwargs)\n",
    "        \n",
    "        # TFDistilBertModel's output[0] is of shape (batch_size, sequence_length, hidden_size)\n",
    "        # We take only for the [CLS].\n",
    "        \n",
    "        sequence_output = outputs[0]  # shape = (batch_size, seq_len, hidden_dim)\n",
    "        pooled_output = sequence_output[:, 0, :]  # shape = (batch_size, hidden_dim)\n",
    "        \n",
    "        return sequence_output, pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    \"bert\": TFBertForNQ,\n",
    "    \"distilbert\": TFDistilBertForNQ\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model(model_name):\n",
    "    \n",
    "    pretrained_path = os.path.join(FLAGS.model_dir, model_name)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_path)\n",
    "    \n",
    "    model_type = model_name.split(\"-\")[0]\n",
    "    if model_type not in model_mapping:\n",
    "        raise ValueError(\"Model definition not found.\")\n",
    "    \n",
    "    model_class = model_mapping[model_type]\n",
    "    model = model_class.from_pretrained(pretrained_path)\n",
    "    \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(name):\n",
    "\n",
    "    loss = tf.keras.metrics.Mean(name=name+'_loss')\n",
    "\n",
    "    loss_short_start_pos = tf.keras.metrics.Mean(name=name+'_loss_short_start_pos')\n",
    "    loss_short_end_pos = tf.keras.metrics.Mean(name=name+'_loss_short_end_pos')\n",
    "    loss_short_ans_type = tf.keras.metrics.Mean(name=name+'_loss_short_ans_type')\n",
    "    \n",
    "    acc = tf.keras.metrics.SparseCategoricalAccuracy(name=name+'_acc')\n",
    "    \n",
    "    acc_short_start_pos = tf.keras.metrics.SparseCategoricalAccuracy(name=name+'_acc_short_start_pos')\n",
    "    acc_short_end_pos = tf.keras.metrics.SparseCategoricalAccuracy(name=name+'_acc_short_end_pos')\n",
    "    acc_short_ans_type = tf.keras.metrics.SparseCategoricalAccuracy(name=name+'_acc_short_ans_type')\n",
    "    \n",
    "    return loss, loss_short_start_pos, loss_short_end_pos, loss_short_ans_type, acc, acc_short_start_pos, acc_short_end_pos, acc_short_ans_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.PolynomialDecay):\n",
    "    \n",
    "    def __init__(self,\n",
    "      initial_learning_rate,\n",
    "      decay_steps,\n",
    "      end_learning_rate=0.0001,\n",
    "      power=1.0,\n",
    "      cycle=False,\n",
    "      name=None,\n",
    "      num_warmup_steps=1000):\n",
    "        \n",
    "        # Since we have a custom __call__() method, we pass cycle=False when calling `super().__init__()` and\n",
    "        # in self.__call__(), we simply do `step = step % self.decay_steps` to have cyclic behavior.\n",
    "        super(CustomSchedule, self).__init__(initial_learning_rate, decay_steps, end_learning_rate, power, cycle=False, name=name)\n",
    "        \n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        \n",
    "        self.cycle = tf.constant(cycle, dtype=tf.bool)\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        \"\"\" `step` is actually the step index, starting at 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # For cyclic behavior\n",
    "        step = tf.cond(self.cycle and step >= self.decay_steps, lambda: step % self.decay_steps, lambda: step)\n",
    "        \n",
    "        learning_rate = super(CustomSchedule, self).__call__(step)\n",
    "\n",
    "        # Copy (including the comments) from original bert optimizer with minor change.\n",
    "        # Ref: https://github.com/google-research/bert/blob/master/optimization.py#L25\n",
    "        \n",
    "        # Implements linear warmup: if global_step < num_warmup_steps, the\n",
    "        # learning rate will be `global_step / num_warmup_steps * init_lr`.\n",
    "        if self.num_warmup_steps > 0:\n",
    "            \n",
    "            steps_int = tf.cast(step, tf.int32)\n",
    "            warmup_steps_int = tf.constant(self.num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "            steps_float = tf.cast(steps_int, tf.float32)\n",
    "            warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "            # The first training step has index (`step`) 0.\n",
    "            # The original code use `steps_float / warmup_steps_float`, which gives `warmup_percent_done` being 0,\n",
    "            # and causing `learning_rate` = 0, which is undesired.\n",
    "            # For this reason, we use `(steps_float + 1) / warmup_steps_float`.\n",
    "            # At `step = warmup_steps_float - 1`, i.e , at the `warmup_steps_float`-th step, \n",
    "            #`learning_rate` is `self.initial_learning_rate`.\n",
    "            warmup_percent_done = (steps_float + 1) / warmup_steps_float\n",
    "            \n",
    "            warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n",
    "\n",
    "            is_warmup = tf.cast(steps_int < warmup_steps_int, tf.float32)\n",
    "            learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "                        \n",
    "        return learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_steps: 1932\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = int(FLAGS.epochs * FLAGS.num_train_examples / FLAGS.train_batch_size)\n",
    "print('num_train_steps:', num_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name './bert-large-uncased-whole-word-masking-finetuned-squad' was not found in tokenizers model name list (bert-base-cased, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-uncased, bert-base-german-cased, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased, bert-base-multilingual-cased, bert-base-multilingual-uncased, bert-base-german-dbmdz-cased, bert-large-uncased, bert-base-cased-finetuned-mrpc, bert-large-uncased-whole-word-masking, bert-base-german-dbmdz-uncased, bert-large-cased-whole-word-masking, bert-base-chinese). We assumed './bert-large-uncased-whole-word-masking-finetuned-squad' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-332148c323b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_nq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-0a5c928143aa>\u001b[0m in \u001b[0;36mget_pretrained_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpretrained_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                         list(cls.vocab_files_names.values())))\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# Get files from url, cache, or disk depending on the case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Model name './bert-large-uncased-whole-word-masking-finetuned-squad' was not found in tokenizers model name list (bert-base-cased, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-uncased, bert-base-german-cased, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased, bert-base-multilingual-cased, bert-base-multilingual-uncased, bert-base-german-dbmdz-cased, bert-large-uncased, bert-base-cased-finetuned-mrpc, bert-large-uncased-whole-word-masking, bert-base-german-dbmdz-uncased, bert-large-cased-whole-word-masking, bert-base-chinese). We assumed './bert-large-uncased-whole-word-masking-finetuned-squad' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "with tpu_strategy.scope():\n",
    "\n",
    "    # Model\n",
    "    bert_tokenizer, bert_nq = get_pretrained_model(FLAGS.model_name)\n",
    "\n",
    "    # Metric\n",
    "    train_loss, train_loss_short_start_pos, train_loss_short_end_pos, train_loss_short_ans_type, train_acc, train_acc_short_start_pos, train_acc_short_end_pos, train_acc_short_ans_type = get_metrics(\"train\")\n",
    "\n",
    "    # Loss\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    def loss_function(nq_labels, nq_logits):\n",
    "        (short_start_pos_labels, short_end_pos_labels, short_answer_type_labels) = nq_labels\n",
    "        (short_start_pos_logits, short_end_pos_logits, short_answer_type_logits) = nq_logits\n",
    "\n",
    "        loss_short_start_pos = loss_object(short_start_pos_labels, short_start_pos_logits)\n",
    "        loss_short_end_pos = loss_object(short_end_pos_labels, short_end_pos_logits)\n",
    "        loss_short_ans_type = loss_object(short_answer_type_labels, short_answer_type_logits)\n",
    "\n",
    "        loss_short_start_pos = tf.nn.compute_average_loss(loss_short_start_pos, global_batch_size=FLAGS.train_batch_size)\n",
    "        loss_short_end_pos = tf.nn.compute_average_loss(loss_short_end_pos, global_batch_size=FLAGS.train_batch_size)\n",
    "        loss_short_ans_type = tf.nn.compute_average_loss(loss_short_ans_type, global_batch_size=FLAGS.train_batch_size)\n",
    "\n",
    "        loss = (loss_short_start_pos + loss_short_end_pos + loss_short_ans_type) / 3.0\n",
    "\n",
    "        return loss, loss_short_start_pos, loss_short_end_pos, loss_short_ans_type\n",
    "\n",
    "\n",
    "    learning_rate = CustomSchedule(\n",
    "        initial_learning_rate=FLAGS.init_learning_rate,\n",
    "        decay_steps=num_train_steps,\n",
    "        end_learning_rate=FLAGS.init_learning_rate,\n",
    "        power=1.0,\n",
    "        cycle=FLAGS.cyclic_learning_rate,\n",
    "        num_warmup_steps=FLAGS.num_warmup_steps\n",
    "    )\n",
    "\n",
    "    decay_var_list = []\n",
    "    for i in range(len(bert_nq.trainable_variables)):\n",
    "        name = bert_nq.trainable_variables[i].name\n",
    "        if any(x in name for x in [\"LayerNorm\", \"layer_norm\", \"bias\"]):\n",
    "            decay_var_list.append(name)\n",
    "\n",
    "    optimizer = AdamW(weight_decay=FLAGS.init_weight_decay_rate, learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay_var_list=decay_var_list)\n",
    "\n",
    "    input_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    ]\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=input_signature)\n",
    "    def train_step(input_ids, input_masks, segment_ids, short_start_pos_labels, short_end_pos_labels, short_answer_type_labels):\n",
    "\n",
    "        nq_inputs = (input_ids, input_masks, segment_ids)\n",
    "        nq_labels = (short_start_pos_labels, short_end_pos_labels, short_answer_type_labels)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            nq_logits = bert_nq(nq_inputs, training=True)\n",
    "            loss, loss_short_start_pos, loss_short_end_pos, loss_short_ans_type = loss_function(nq_labels, nq_logits)\n",
    "\n",
    "        gradients = tape.gradient(loss, bert_nq.trainable_variables)\n",
    "\n",
    "        (short_start_pos_logits, short_end_pos_logits, short_answer_type_logits) = nq_logits\n",
    "\n",
    "        train_acc.update_state(short_start_pos_labels, short_start_pos_logits)\n",
    "        train_acc.update_state(short_end_pos_labels, short_end_pos_logits)\n",
    "        train_acc.update_state(short_answer_type_labels, short_answer_type_logits)\n",
    "\n",
    "        train_acc_short_start_pos.update_state(short_start_pos_labels, short_start_pos_logits)\n",
    "        train_acc_short_end_pos.update_state(short_end_pos_labels, short_end_pos_logits)\n",
    "        train_acc_short_ans_type.update_state(short_answer_type_labels, short_answer_type_logits)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, bert_nq.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "\n",
    "        train_loss_short_start_pos(loss_short_start_pos)\n",
    "        train_loss_short_end_pos(loss_short_end_pos)\n",
    "        train_loss_short_ans_type(loss_short_ans_type)\n",
    "\n",
    "\n",
    "    # `experimental_run_v2` replicates the provided computation and runs it with the distributed input.\n",
    "    @tf.function\n",
    "    def distributed_train_step(dataset_inputs):\n",
    "\n",
    "        features, targets = dataset_inputs\n",
    "        (input_ids, input_masks, segment_ids) = (features['input_ids'], features['input_mask'], features['segment_ids'])\n",
    "        (short_start_pos_labels, short_end_pos_labels, short_answer_type_labels) = (targets['short_start_positions'], targets['short_end_positions'], targets['short_answer_types'])\n",
    "\n",
    "        tpu_strategy.experimental_run_v2(train_step, args=(input_ids, input_masks, segment_ids, short_start_pos_labels, short_end_pos_labels, short_answer_type_labels))\n",
    "\n",
    "checkpoint_path = FLAGS.input_checkpoint_dir + FLAGS.model_name + \"/\"\n",
    "ckpt = tf.train.Checkpoint(model=bert_nq)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1c0fc7a7aba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_path' is not defined"
     ]
    }
   ],
   "source": [
    "pretrained_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
