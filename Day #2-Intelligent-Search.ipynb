{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intelligent Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyprince999/100-Days-Of-ML/blob/master/Day%20%232-Intelligent-Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Me-79mKEsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Date: 6th January 2020\n",
        "\n",
        "# Source: Taken verbatim from DataHack Summit 2019 \n",
        "# HackSession - Enabling Intelligent Search using Q&A models - Abhishek Jha, Priya Shree & Atul Singh (PhD) \n",
        "\n",
        "#!pip install deeppavlov"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMwLGGJMEmv",
        "colab_type": "code",
        "outputId": "854f29ae-3ed2-4d86-d905-bfb7c3545a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JAjXylCMgIO",
        "colab_type": "code",
        "outputId": "7ffade28-a373-4eca-8920-ad0fe2e3e882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!ls /root/nltk_data/corpora/gutenberg/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "austen-emma.txt\t\t carroll-alice.txt\t  README\n",
            "austen-persuasion.txt\t chesterton-ball.txt\t  shakespeare-caesar.txt\n",
            "austen-sense.txt\t chesterton-brown.txt\t  shakespeare-hamlet.txt\n",
            "bible-kjv.txt\t\t chesterton-thursday.txt  shakespeare-macbeth.txt\n",
            "blake-poems.txt\t\t edgeworth-parents.txt\t  whitman-leaves.txt\n",
            "bryant-stories.txt\t melville-moby_dick.txt\n",
            "burgess-busterbrown.txt  milton-paradise.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTSC6133IQWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqtoB4jLJu6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVrWpXxaJ9yM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from deeppavlov import build_model, configs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgIyjH0qKWu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "book_name = 'carroll-alice.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agu_FpYyLA_W",
        "colab_type": "code",
        "outputId": "bbc6e3ae-d506-4d1c-8c2a-738e9a17cc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "display(HTML('<h2> No. of paragraphs: ' +str(len(gutenberg.paras(book_name))) + ' </h2> '))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2> No. of paragraphs: 817 </h2> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nfq0TGXLiKE",
        "colab_type": "code",
        "outputId": "7e31b962-72f9-496b-e138-332a03eb2f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "display(HTML('<h2> No. of words: ' +str(len(gutenberg.words(book_name))) + ' </h2> '))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2> No. of words: 34110 </h2> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QyzD3IlNs-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gutenberg.paras(book_name)[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4VTL1DPHtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "para_list =[]\n",
        "\n",
        "for para in gutenberg.paras(book_name):\n",
        "  sentence_list = []\n",
        "\n",
        "  for sentence in para:\n",
        "    sentence_list.append(TreebankWordDetokenizer().detokenize(sentence))\n",
        "\n",
        "  para_list.append(\" \".join((sent for sent in sentence_list))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2DY9SLQXzX",
        "colab_type": "code",
        "outputId": "5ee6ddd4-1ae4-40cb-9aa9-182055283a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "para_list[4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself ,' Oh dear! Oh dear! I shall be late!' ( when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural ); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT - POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat - pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit - hole under the hedge.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13yQOADTQlFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?TreebankWordDetokenizer().detokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeIUXIedQq3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zafbIvdmSGoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vec_total_text = count_vectorizer.fit_transform(para_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh7FUQMITSv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "question = \"What did the rabbit take out of his waistcoat pocket\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Ol_YlZUgEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vec_question = count_vectorizer.transform([question])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N65CdNNUq9f",
        "colab_type": "code",
        "outputId": "5ae9b1f3-c037-4cd8-8497-f1f8afa9e66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.shape(count_vec_total_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(817, 10658)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz0arbnPU3eB",
        "colab_type": "code",
        "outputId": "94da0754-43a0-4b5b-c4fb-8d5c7297d296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.shape(count_vec_question))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 10658)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVkF9yDkU6vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distance_array = cosine_similarity(count_vec_question, count_vec_total_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xr7Tce_V1KI",
        "colab_type": "code",
        "outputId": "a79638fa-260c-4532-8c7c-eb74e3839434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(distance_array)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 817)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAFlazVcVo2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distance_array = distance_array[0] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trPLsVLxVtYl",
        "colab_type": "code",
        "outputId": "afae6fcb-762b-4150-ebf2-402b18202176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "distance_array"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.18257419, 0.        , 0.06085806, 0.42802583,\n",
              "       0.        , 0.07106691, 0.0402259 , 0.        , 0.        ,\n",
              "       0.        , 0.0255655 , 0.0789337 , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.09774528, 0.        , 0.02879561, 0.        , 0.        ,\n",
              "       0.        , 0.04778185, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.03178209, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.05227084, 0.        , 0.        ,\n",
              "       0.10540926, 0.        , 0.07332356, 0.        , 0.        ,\n",
              "       0.08908708, 0.05832118, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.06711561, 0.        , 0.09901475,\n",
              "       0.        , 0.        , 0.        , 0.07106691, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.13608276, 0.03910309,\n",
              "       0.05407381, 0.04536092, 0.04233338, 0.        , 0.05504819,\n",
              "       0.        , 0.        , 0.04188539, 0.        , 0.        ,\n",
              "       0.        , 0.09829464, 0.05954913, 0.10127394, 0.0758098 ,\n",
              "       0.09365858, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.03874921, 0.        , 0.        ,\n",
              "       0.06375767, 0.        , 0.15430335, 0.08908708, 0.06900656,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.02938635, 0.        , 0.        , 0.05407381,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07856742, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05407381,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.03487901,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.08164966,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.05954913, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.04481107,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.06711561, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04845016, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.06711561, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.04536092, 0.12309149, 0.        , 0.        ,\n",
              "       0.        , 0.04327423, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.07332356, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.09901475, 0.        ,\n",
              "       0.        , 0.        , 0.08908708, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.14664712, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.1132277 , 0.        ,\n",
              "       0.        , 0.        , 0.16329932, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.05407381, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04376881, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.06475239,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.15430335, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.08908708, 0.        , 0.06537205,\n",
              "       0.        , 0.27216553, 0.        , 0.11433239, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.0531494 ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.11215443, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07856742, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.04188539, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.13608276, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.15430335, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.08908708, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.17817416,\n",
              "       0.09901475, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.12309149, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.04481107, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.08908708, 0.        , 0.05143445, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.03946685, 0.        , 0.14664712, 0.        , 0.        ,\n",
              "       0.        , 0.06375767, 0.        , 0.05063697, 0.        ,\n",
              "       0.09365858, 0.        , 0.        , 0.12309149, 0.08908708,\n",
              "       0.        , 0.12309149, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07106691, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.18257419, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.13608276, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.08512565, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.11215443, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.05954913, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.08512565, 0.        ,\n",
              "       0.09901475, 0.        , 0.        , 0.07106691, 0.        ,\n",
              "       0.10540926, 0.        , 0.09901475, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.1132277 , 0.        ,\n",
              "       0.15430335, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.08908708, 0.06900656, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.03910309,\n",
              "       0.        , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmVvzipAV5G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_n = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvTyRCqcV-mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_passages = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB1gzDHHWBSH",
        "colab_type": "code",
        "outputId": "8ca38b3c-0ab9-4fda-c449-ee10e5d8c48f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "distance_array.argsort()[::-1][:top_n]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  4, 456,   1, 712, 574])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkC0scu3WPTW",
        "colab_type": "code",
        "outputId": "a2f6bfb8-8435-4c6e-b764-bd43fc52f7f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for idx in  [4, 456, 1, 712, 574]:\n",
        "  print(\" {:03.2f}\".format(distance_array[idx]))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 0.43\n",
            " 0.27\n",
            " 0.18\n",
            " 0.18\n",
            " 0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD8ErGVBWSOn",
        "colab_type": "code",
        "outputId": "e664bbef-9696-44f9-9752-b40e6db1a5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "for idx in distance_array.argsort()[::-1][:top_n]:\n",
        "  print(idx, ' || ', para_list[idx] )\n",
        "  print(\"=\"*20)\n",
        "  top_passages.append(para_list[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4  ||  There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself ,' Oh dear! Oh dear! I shall be late!' ( when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural ); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT - POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat - pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit - hole under the hedge.\n",
            "====================\n",
            "456  ||  ' Did you say \" What a pity!\"?' the Rabbit asked.\n",
            "====================\n",
            "1  ||  CHAPTER I. Down the Rabbit - Hole\n",
            "====================\n",
            "712  ||  ' You did!' said the Hatter.\n",
            "====================\n",
            "574  ||  ' So he did, so he did ,' said the Gryphon, sighing in his turn; and both creatures hid their faces in their paws.\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ETHmWxd6fP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m deeppavlov install squad_bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGq9EpQHXhl4",
        "colab_type": "code",
        "outputId": "6cf202e4-9f53-4b0c-a767-865572e5ed22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        }
      },
      "source": [
        "from deeppavlov import build_model, configs\n",
        "\n",
        "model = build_model(configs.squad.squad_bert, download = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-06 10:47:25.703 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip download because of matching hashes\n",
            "2020-01-06 10:47:26.988 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_bert.tar.gz download because of matching hashes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:81: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:178: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:166: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:94: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-06 10:47:52.686 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/squad_bert/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/squad_bert/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl4hVOZAYePi",
        "colab_type": "code",
        "outputId": "5e6ca425-dac5-4f48-8731-a7b1178261f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "predicted_answer_list = []\n",
        "\n",
        "for passage in top_passages:\n",
        "  predicted_answer = model([passage], [question])\n",
        "  predicted_answer_list.append(predicted_answer)\n",
        "  print(predicted_answer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['a watch'], [562], [135285.109375]]\n",
            "[['the Rabbit asked.'], [32], [0.6844851970672607]]\n",
            "[['CHAPTER I. Down the Rabbit - Hole'], [0], [0.4659876227378845]]\n",
            "[['You did'], [2], [4.523017406463623]]\n",
            "[['both creatures hid their faces in their paws'], [69], [761.21533203125]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzjhJaJWeb0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEl9BKoEe4zN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVIMTJjqe49G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR2qFgEufm7w",
        "colab_type": "code",
        "outputId": "d8785099-ebcb-482f-c314-4094c73365ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.32.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.16.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=ec7ca69f24f002b2db664791076920a74942e5f03e73c304a92d0eafe7daf6b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u151G4fHe5k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x0KplxrfbJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_text = \" [CLS] \" + question + \" [SEP] \" + top_passages[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3eGmPyk-Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_text = tokenizer.tokenize(input_text)\n",
        "\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZQQgMW1lRAo",
        "colab_type": "code",
        "outputId": "c1fbdb10-478a-4e4e-98b2-22df87cdccb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(top_passages[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself ,' Oh dear! Oh dear! I shall be late!' ( when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural ); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT - POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat - pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit - hole under the hedge.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFRPl0M_lUbK",
        "colab_type": "code",
        "outputId": "9ae09051-1f38-4cce-a85d-bb05cde85c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'what', 'did', 'the', 'rabbit', 'take', 'out', 'of', 'his', 'waist', '##coat', 'pocket', '[SEP]', 'there', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'rabbit', 'say', 'to', 'itself', ',', \"'\", 'oh', 'dear', '!', 'oh', 'dear', '!', 'i', 'shall', 'be', 'late', '!', \"'\", '(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ')', ';', 'but', 'when', 'the', 'rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waist', '##coat', '-', 'pocket', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waist', '##coat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPglm3iZlnFh",
        "colab_type": "code",
        "outputId": "f5b0ab40-8a48-4df1-ad9f-945579f1c842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for tup in zip(tokenized_text, input_ids):\n",
        "  print(tup)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('[CLS]', 101)\n",
            "('what', 2054)\n",
            "('did', 2106)\n",
            "('the', 1996)\n",
            "('rabbit', 10442)\n",
            "('take', 2202)\n",
            "('out', 2041)\n",
            "('of', 1997)\n",
            "('his', 2010)\n",
            "('waist', 5808)\n",
            "('##coat', 16531)\n",
            "('pocket', 4979)\n",
            "('[SEP]', 102)\n",
            "('there', 2045)\n",
            "('was', 2001)\n",
            "('nothing', 2498)\n",
            "('so', 2061)\n",
            "('very', 2200)\n",
            "('remarkable', 9487)\n",
            "('in', 1999)\n",
            "('that', 2008)\n",
            "(';', 1025)\n",
            "('nor', 4496)\n",
            "('did', 2106)\n",
            "('alice', 5650)\n",
            "('think', 2228)\n",
            "('it', 2009)\n",
            "('so', 2061)\n",
            "('very', 2200)\n",
            "('much', 2172)\n",
            "('out', 2041)\n",
            "('of', 1997)\n",
            "('the', 1996)\n",
            "('way', 2126)\n",
            "('to', 2000)\n",
            "('hear', 2963)\n",
            "('the', 1996)\n",
            "('rabbit', 10442)\n",
            "('say', 2360)\n",
            "('to', 2000)\n",
            "('itself', 2993)\n",
            "(',', 1010)\n",
            "(\"'\", 1005)\n",
            "('oh', 2821)\n",
            "('dear', 6203)\n",
            "('!', 999)\n",
            "('oh', 2821)\n",
            "('dear', 6203)\n",
            "('!', 999)\n",
            "('i', 1045)\n",
            "('shall', 4618)\n",
            "('be', 2022)\n",
            "('late', 2397)\n",
            "('!', 999)\n",
            "(\"'\", 1005)\n",
            "('(', 1006)\n",
            "('when', 2043)\n",
            "('she', 2016)\n",
            "('thought', 2245)\n",
            "('it', 2009)\n",
            "('over', 2058)\n",
            "('afterwards', 5728)\n",
            "(',', 1010)\n",
            "('it', 2009)\n",
            "('occurred', 4158)\n",
            "('to', 2000)\n",
            "('her', 2014)\n",
            "('that', 2008)\n",
            "('she', 2016)\n",
            "('ought', 11276)\n",
            "('to', 2000)\n",
            "('have', 2031)\n",
            "('wondered', 4999)\n",
            "('at', 2012)\n",
            "('this', 2023)\n",
            "(',', 1010)\n",
            "('but', 2021)\n",
            "('at', 2012)\n",
            "('the', 1996)\n",
            "('time', 2051)\n",
            "('it', 2009)\n",
            "('all', 2035)\n",
            "('seemed', 2790)\n",
            "('quite', 3243)\n",
            "('natural', 3019)\n",
            "(')', 1007)\n",
            "(';', 1025)\n",
            "('but', 2021)\n",
            "('when', 2043)\n",
            "('the', 1996)\n",
            "('rabbit', 10442)\n",
            "('actually', 2941)\n",
            "('took', 2165)\n",
            "('a', 1037)\n",
            "('watch', 3422)\n",
            "('out', 2041)\n",
            "('of', 1997)\n",
            "('its', 2049)\n",
            "('waist', 5808)\n",
            "('##coat', 16531)\n",
            "('-', 1011)\n",
            "('pocket', 4979)\n",
            "(',', 1010)\n",
            "('and', 1998)\n",
            "('looked', 2246)\n",
            "('at', 2012)\n",
            "('it', 2009)\n",
            "(',', 1010)\n",
            "('and', 1998)\n",
            "('then', 2059)\n",
            "('hurried', 9520)\n",
            "('on', 2006)\n",
            "(',', 1010)\n",
            "('alice', 5650)\n",
            "('started', 2318)\n",
            "('to', 2000)\n",
            "('her', 2014)\n",
            "('feet', 2519)\n",
            "(',', 1010)\n",
            "('for', 2005)\n",
            "('it', 2009)\n",
            "('flashed', 8373)\n",
            "('across', 2408)\n",
            "('her', 2014)\n",
            "('mind', 2568)\n",
            "('that', 2008)\n",
            "('she', 2016)\n",
            "('had', 2018)\n",
            "('never', 2196)\n",
            "('before', 2077)\n",
            "('seen', 2464)\n",
            "('a', 1037)\n",
            "('rabbit', 10442)\n",
            "('with', 2007)\n",
            "('either', 2593)\n",
            "('a', 1037)\n",
            "('waist', 5808)\n",
            "('##coat', 16531)\n",
            "('-', 1011)\n",
            "('pocket', 4979)\n",
            "(',', 1010)\n",
            "('or', 2030)\n",
            "('a', 1037)\n",
            "('watch', 3422)\n",
            "('to', 2000)\n",
            "('take', 2202)\n",
            "('out', 2041)\n",
            "('of', 1997)\n",
            "('it', 2009)\n",
            "(',', 1010)\n",
            "('and', 1998)\n",
            "('burning', 5255)\n",
            "('with', 2007)\n",
            "('curiosity', 10628)\n",
            "(',', 1010)\n",
            "('she', 2016)\n",
            "('ran', 2743)\n",
            "('across', 2408)\n",
            "('the', 1996)\n",
            "('field', 2492)\n",
            "('after', 2044)\n",
            "('it', 2009)\n",
            "(',', 1010)\n",
            "('and', 1998)\n",
            "('fortunately', 14599)\n",
            "('was', 2001)\n",
            "('just', 2074)\n",
            "('in', 1999)\n",
            "('time', 2051)\n",
            "('to', 2000)\n",
            "('see', 2156)\n",
            "('it', 2009)\n",
            "('pop', 3769)\n",
            "('down', 2091)\n",
            "('a', 1037)\n",
            "('large', 2312)\n",
            "('rabbit', 10442)\n",
            "('-', 1011)\n",
            "('hole', 4920)\n",
            "('under', 2104)\n",
            "('the', 1996)\n",
            "('hedge', 17834)\n",
            "('.', 1012)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVDQroBelyGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# identify the question & the answer paragraph\n",
        "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naUi7YG_mPcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_score, end_score = model(torch.tensor([input_ids]), token_type_ids= torch.tensor([token_type_ids]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q44Mqzcm866",
        "colab_type": "code",
        "outputId": "0b5ab250-337b-4e45-9ef7-349f5ac36416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(start_score[0]), len(input_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(183, 183)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNoqdaX2nC-J",
        "colab_type": "code",
        "outputId": "e2ee3ce9-88f4-471b-e353-94618fd7c43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "start_score[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-5.1969, -7.5432, -8.4612, -7.9589, -8.0555, -7.3838, -7.9336, -7.9534,\n",
              "        -7.6459, -8.7929, -9.5364, -9.9329, -5.1969, -7.5316, -7.8964, -6.0435,\n",
              "        -7.4493, -8.3221, -7.5872, -6.9114, -6.6398, -8.5586, -7.5670, -7.7474,\n",
              "        -5.3059, -7.4327, -7.1054, -7.5455, -8.3294, -8.0227, -6.9036, -8.0763,\n",
              "        -8.5485, -8.2907, -7.6083, -8.1075, -7.6163, -6.0191, -8.5698, -8.4949,\n",
              "        -7.5469, -8.5402, -5.6716, -5.2807, -7.7538, -8.1537, -6.0293, -7.6878,\n",
              "        -8.1158, -5.4844, -7.5109, -7.8663, -6.3497, -8.6834, -8.1787, -8.0117,\n",
              "        -7.2577, -7.6060, -7.7316, -7.8688, -8.6228, -7.6330, -8.4917, -7.3532,\n",
              "        -7.3277, -8.4594, -7.7542, -8.4035, -7.3356, -8.2420, -8.5340, -8.7207,\n",
              "        -7.5331, -8.9373, -7.6748, -8.7764, -7.8257, -6.6288, -6.5740, -7.9892,\n",
              "        -6.4077, -7.5728, -6.3276, -6.6751, -6.0519, -7.6977, -6.4876, -4.2391,\n",
              "        -0.2670,  2.7878,  1.1234,  0.3546,  2.5449,  5.0744,  3.7283, -2.7159,\n",
              "        -3.4897, -1.6514, -0.7780, -4.5677, -6.4190, -2.6466, -5.7070, -4.5771,\n",
              "        -2.1173, -6.0080, -4.1508, -6.3143, -5.5785, -4.1206, -3.1319, -4.9549,\n",
              "        -5.3581, -1.8787, -5.7820, -7.4148, -7.0195, -5.8570, -7.1535, -5.7944,\n",
              "        -4.9065, -5.8920, -7.4256, -6.9226, -6.4530, -5.6167, -3.7782, -6.0547,\n",
              "        -3.3057, -6.5160, -6.3240, -4.4823, -2.8779, -4.8685, -5.3115, -1.6927,\n",
              "        -1.9251, -6.7586, -7.5132, -4.1423, -7.8494, -6.2073,  2.3957,  2.1293,\n",
              "        -4.9341, -5.2648, -6.2101, -7.0480, -6.2051, -7.5519, -7.2893, -5.0114,\n",
              "        -8.1506, -6.0261, -8.3104, -5.3995, -5.8676, -7.4670, -8.2549, -7.5324,\n",
              "        -6.3820, -6.8388, -8.1540, -7.5573, -5.9261, -6.7060, -6.8440, -7.4701,\n",
              "        -7.4945, -7.6241, -6.9801, -6.2888, -5.6583, -7.3927, -7.1321, -6.6284,\n",
              "        -5.4681, -8.4563, -7.5201, -7.6940, -8.6548, -7.0867, -7.2965],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l67e4hDnPeN",
        "colab_type": "code",
        "outputId": "066234df-0f82-46f7-95a8-b28e3f319c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.argmax(start_score), torch.max(start_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(93), tensor(5.0744, grad_fn=<MaxBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRupkXIUnzZR",
        "colab_type": "code",
        "outputId": "2846980c-bb48-41fb-a0e7-8022405e7621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.argmax(end_score), torch.max(end_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(94), tensor(5.2814, grad_fn=<MaxBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Lnn29gn5PW",
        "colab_type": "code",
        "outputId": "fc7afa85-609c-45fc-90fb-811936cfe948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "torch.argmax(torch.nn.Softmax()(start_score[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(93)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTqE0XfCoLC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u70wGKlEobtQ",
        "colab_type": "code",
        "outputId": "120787c4-37e7-4b07-d29b-3b574a7a7677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(all_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'what', 'did', 'the', 'rabbit', 'take', 'out', 'of', 'his', 'waist', '##coat', 'pocket', '[SEP]', 'there', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'rabbit', 'say', 'to', 'itself', ',', \"'\", 'oh', 'dear', '!', 'oh', 'dear', '!', 'i', 'shall', 'be', 'late', '!', \"'\", '(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ')', ';', 'but', 'when', 'the', 'rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waist', '##coat', '-', 'pocket', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waist', '##coat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9UsjenoodTi",
        "colab_type": "code",
        "outputId": "b0784f9b-f23c-47f7-a719-027b509ce2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted_answer = \" \".join(all_tokens[torch.argmax(start_score): torch.argmax(end_score)+ 1]) \n",
        "predicted_answer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a watch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYWFIQvEo9p2",
        "colab_type": "code",
        "outputId": "75a5626c-868c-44cd-f98a-87c17af67574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rcZ6O8PpBCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}